{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a0da01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beast/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from collections import defaultdict, OrderedDict\n",
    "import ipdb\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab78cb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "image_size = 32\n",
    "transforms = {\n",
    "    \"train\": Compose([\n",
    "        RandomCrop(image_size, padding=4),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "    ]),\n",
    "    \"test\": ToTensor(),\n",
    "}\n",
    "dataset = {}\n",
    "for split in [\"train\", \"test\"]:\n",
    "  dataset[split] = CIFAR10(\n",
    "    root=\"data/cifar10\",\n",
    "    train=(split == \"train\"),\n",
    "    download=True,\n",
    "    transform=transforms[split],\n",
    "  )\n",
    "dataloader = {}\n",
    "for split in ['train', 'test']:\n",
    "  dataloader[split] = DataLoader(\n",
    "    dataset[split],\n",
    "    batch_size=512,\n",
    "    shuffle=(split == 'train'),\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "  )\n",
    "\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "  ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "  def __init__(self) -> None:\n",
    "      super().__init__()\n",
    "\n",
    "      layers = []\n",
    "      counts = defaultdict(int)\n",
    "\n",
    "      def add(name: str, layer: nn.Module) -> None:\n",
    "          layers.append((f\"{name}{counts[name]}\", layer))\n",
    "          counts[name] += 1\n",
    "\n",
    "      in_channels = 3\n",
    "      for x in self.ARCH:\n",
    "          if x != 'M':\n",
    "              # conv-bn-relu\n",
    "              add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n",
    "              add(\"bn\", nn.BatchNorm2d(x))\n",
    "              add(\"relu\", nn.ReLU(True))\n",
    "              in_channels = x\n",
    "          else:\n",
    "              # maxpool\n",
    "              add(\"pool\", nn.MaxPool2d(2))\n",
    "\n",
    "      self.backbone = nn.Sequential(OrderedDict(layers))\n",
    "      self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "      # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]\n",
    "      x = self.backbone(x)\n",
    "\n",
    "      # avgpool: [N, 512, 2, 2] => [N, 512]\n",
    "      x = x.mean([2, 3])\n",
    "\n",
    "      # classifier: [N, 512] => [N, 10]\n",
    "      x = self.classifier(x)\n",
    "      return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b770f4c4-22a0-4491-8447-f16b86904ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VGG(nn.Module):\n",
    "  ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "  def __init__(self) -> None:\n",
    "    super().__init__()\n",
    "\n",
    "    layers = []\n",
    "    counts = defaultdict(int)\n",
    "\n",
    "    def add(name: str, layer: nn.Module) -> None:\n",
    "      layers.append((f\"{name}{counts[name]}\", layer))\n",
    "      counts[name] += 1\n",
    "\n",
    "    in_channels = 3\n",
    "    for x in self.ARCH:\n",
    "      if x != 'M':\n",
    "        # conv-bn-relu\n",
    "        add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n",
    "        add(\"bn\", nn.BatchNorm2d(x))\n",
    "        add(\"relu\", nn.ReLU(True))\n",
    "        in_channels = x\n",
    "      else:\n",
    "        # maxpool\n",
    "        add(\"pool\", nn.MaxPool2d(2))\n",
    "\n",
    "    self.backbone = nn.Sequential(OrderedDict(layers))\n",
    "    self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]\n",
    "    x = self.backbone(x)\n",
    "\n",
    "    # avgpool: [N, 512, 2, 2] => [N, 512]\n",
    "    x = x.mean([2, 3])\n",
    "\n",
    "    # classifier: [N, 512] => [N, 10]\n",
    "    x = self.classifier(x)\n",
    "    return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e70e3bf-d8a2-4b59-91f0-6e92d555961a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e458a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class ModelAnalyzer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.mapped_layers = self.layer_mapping(model)\n",
    " \n",
    "\n",
    "    \n",
    "    def name_fixer(self, names):\n",
    "        \"\"\"\n",
    "        Fix the names by removing the indices in square brackets.\n",
    "        Args:\n",
    "        names (list): List of names.\n",
    "\n",
    "        Returns:\n",
    "        list: List of fixed names.\n",
    "        \"\"\"\n",
    "        return_list = []\n",
    "        for string in names:\n",
    "            matches = re.finditer(r'\\.\\[(\\d+)\\]', string)\n",
    "            pop_list = [m.start(0) for m in matches]\n",
    "            pop_list.sort(reverse=True)\n",
    "            if len(pop_list) > 0:\n",
    "                string = list(string)\n",
    "                for pop_id in pop_list:\n",
    "                    string.pop(pop_id)\n",
    "                string = ''.join(string)\n",
    "            return_list.append(string)\n",
    "        return return_list\n",
    "\n",
    "\n",
    "    def get_all_layers(self, model, parent_name=''):\n",
    "        layers = []\n",
    "        for name, module in model.named_children():\n",
    "            full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "            test_name = \"model.\" + full_name\n",
    "            try:\n",
    "                eval(test_name)\n",
    "                layers.append((full_name, module))\n",
    "            except:\n",
    "                layers.append((self.reformat_layer_name(full_name), module))\n",
    "            if isinstance(module, nn.Module):\n",
    "                layers.extend(self.get_all_layers(module, parent_name=full_name))\n",
    "        return layers\n",
    "\n",
    "\n",
    "    def reformat_layer_name(self, str_data):\n",
    "        try:\n",
    "            split_data = str_data.split('.')\n",
    "            for ind in range(len(split_data)):\n",
    "                data = split_data[ind]\n",
    "                if (data.isdigit()):\n",
    "                    split_data[ind] = \"[\" + data + \"]\"\n",
    "            final_string = '.'.join(split_data)\n",
    "\n",
    "            iters_a = re.finditer(r'[a-zA-Z]\\.\\[', final_string)\n",
    "            indices = [m.start(0) + 1 for m in iters_a]\n",
    "            iters = re.finditer(r'\\]\\.\\[', final_string)\n",
    "            indices.extend([m.start(0) + 1 for m in iters])\n",
    "\n",
    "            final_string = list(final_string)\n",
    "            final_string = [final_string[i] for i in range(len(final_string)) if i not in indices]\n",
    "\n",
    "            str_data = ''.join(final_string)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return str_data\n",
    "\n",
    "\n",
    "    def summary_string_fixed(self, model, all_layers, input_size, model_name=None, batch_size=-1, dtypes=None):\n",
    "        if dtypes is None:\n",
    "            dtypes = [torch.FloatTensor] * len(input_size)\n",
    "\n",
    "        def register_hook(module, module_idx):\n",
    "            def hook(module, input, output):\n",
    "                nonlocal module_idx\n",
    "                m_key = all_layers[module_idx][0]\n",
    "                m_key = model_name + \".\" + m_key\n",
    "\n",
    "                try:\n",
    "                    eval(m_key)\n",
    "                except:\n",
    "                    m_key = name_fixer([m_key])[0]\n",
    "\n",
    "                summary[m_key] = OrderedDict()\n",
    "                summary[m_key][\"type\"] = str(type(module)).split('.')[-1][:-2]\n",
    "                summary[m_key][\"x\"] = input\n",
    "                summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "                summary[m_key][\"input_shape\"][0] = batch_size\n",
    "\n",
    "                if isinstance(output, (list, tuple)):\n",
    "                    summary[m_key][\"y\"] = [\n",
    "                        [-1] + list(o)[1:] for o in output\n",
    "                    ]\n",
    "                    summary[m_key][\"output_shape\"] = [\n",
    "                        [-1] + list(o.size())[1:] for o in output\n",
    "                    ]\n",
    "                else:\n",
    "                    summary[m_key][\"y\"] = list(output)\n",
    "\n",
    "\n",
    "                    summary[m_key][\"output_shape\"] = list(output.size())\n",
    "                    summary[m_key][\"output_shape\"][0] = batch_size\n",
    "\n",
    "                params = 0\n",
    "                if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                    summary[m_key][\"w\"] = module.weight\n",
    "                    params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "                    summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
    "                    summary[m_key][\"weight_shape\"] = module.weight.shape\n",
    "                if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                    summary[m_key][\"b\"] = module.bias\n",
    "                    params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "                summary[m_key][\"nb_params\"] = params\n",
    "\n",
    "            if (\n",
    "                    not isinstance(module, nn.Sequential)\n",
    "                    and not isinstance(module, nn.ModuleList)\n",
    "            ):\n",
    "                hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "        if isinstance(input_size, tuple):\n",
    "            input_size = [input_size]\n",
    "            \n",
    "        model_device = next(iter(model.parameters())).device\n",
    "        x, _ = next(iter(dataloader['test']))\n",
    "        x = x.to(model_device)\n",
    "\n",
    "        summary = OrderedDict()\n",
    "        hooks = []\n",
    "\n",
    "        for module_idx, (layer_name, module) in enumerate(all_layers):\n",
    "            register_hook(module, module_idx)\n",
    "\n",
    "        model(x)\n",
    "\n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "\n",
    "        return summary\n",
    "\n",
    "\n",
    "    def layer_mapping(self, model):\n",
    "        all_layers = self.get_all_layers(model)\n",
    "        x,y = next(iter(dataloader['test']))\n",
    "        model_summary = self.summary_string_fixed(model, all_layers, x.shape, model_name='model')\n",
    "\n",
    "        name_type_shape = []\n",
    "        for key in model_summary.keys():\n",
    "            data = model_summary[key]\n",
    "            if (\"weight_shape\" in data.keys()):\n",
    "                name_type_shape.append([key, data['type'], data['weight_shape'][0]])\n",
    "            else:\n",
    "                name_type_shape.append([key, data['type'], 0 ])\n",
    "        name_type_shape = np.asarray(name_type_shape)\n",
    "\n",
    "        name_list = name_type_shape[:, 0]\n",
    "\n",
    "        r_name_list = np.asarray(name_list)\n",
    "        random_picks = np.random.randint(0, len(r_name_list), 10)\n",
    "        test_name_list = r_name_list[random_picks]\n",
    "        eval_hit = False\n",
    "        for layer in test_name_list:\n",
    "            try:\n",
    "                eval(layer)\n",
    "\n",
    "            except:\n",
    "                eval_hit = True\n",
    "                break\n",
    "        if (eval_hit):\n",
    "            fixed_name_list = name_fixer(r_name_list)\n",
    "            name_type_shape[:, 0] = fixed_name_list\n",
    "\n",
    "        layer_types = name_type_shape[:, 1]\n",
    "        layer_shapes = name_type_shape[:, 2]\n",
    "        mapped_layers = {'model_layer': [], 'Conv2d_BatchNorm2d_ReLU': [], 'Conv2d_BatchNorm2d': [], 'Linear_ReLU': [],\n",
    "                        'Linear_BatchNorm1d': []}\n",
    "\n",
    "        def detect_sequences(lst):\n",
    "            fusing_layers = [\n",
    "                            'Conv2d',\n",
    "                            'BatchNorm2d',\n",
    "                            'ReLU',\n",
    "                            'Linear',\n",
    "                            'BatchNorm1d',\n",
    "                        ]\n",
    "\n",
    "            i = 0\n",
    "            while i < len(lst):\n",
    "\n",
    "                if i + 2 < len(lst) and [l for l in lst[i: i + 3]] == [\n",
    "                    fusing_layers[0],\n",
    "                    fusing_layers[1],\n",
    "                    fusing_layers[2],\n",
    "                ]:\n",
    "                    \n",
    "                    mapped_layers['Conv2d_BatchNorm2d_ReLU'].append(\n",
    "                            np.take(name_list, [i for i in range(i, i + 3)]).tolist()\n",
    "                        )\n",
    "                    i += 3\n",
    "\n",
    "                elif i + 1 < len(lst) and [l for l in lst[i: i + 2]] == [\n",
    "                    fusing_layers[0],\n",
    "                    fusing_layers[1],\n",
    "                ]:\n",
    "\n",
    "                    mapped_layers['Conv2d_BatchNorm2d'].append(\n",
    "                            np.take(name_list, [i for i in range(i, i + 2)]).tolist()\n",
    "                        )\n",
    "                    i += 2\n",
    "\n",
    "                elif i + 1 < len(lst) and [l for l in lst[i: i + 2]] == [\n",
    "                    fusing_layers[3],\n",
    "                    fusing_layers[2],\n",
    "                ]:\n",
    "                    mapped_layers['Linear_ReLU'].append(\n",
    "                        np.take(name_list, [i for i in range(i, i + 2)]).tolist()\n",
    "                    )\n",
    "                    i += 2\n",
    "                elif i + 1 < len(lst) and [l for l in lst[i: i + 2]] == [\n",
    "                    fusing_layers[3],\n",
    "                    fusing_layers[4],\n",
    "                ]:\n",
    "                    mapped_layers['Linear_BatchNorm1d'].append(\n",
    "                        np.take(name_list, [i for i in range(i, i + 2)]).tolist()\n",
    "                    )\n",
    "                    i += 2\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "        def detect_sorting_pairs(name_list):\n",
    "            sorting_pairs = []\n",
    "            idx = 0\n",
    "            while idx < len(name_list):\n",
    "                layer_list = [ layer_name for layer_name in name_list[idx:idx+3]]\n",
    "                \n",
    "                if([ type(eval(l)) for l in layer_list] == [nn.Conv2d, nn.BatchNorm2d,nn.Conv2d]):\n",
    "                    sorting_pairs.append(layer_list)\n",
    "                    idx+=2\n",
    "                elif([ type(eval(l)) for l in layer_list[idx:idx+2]] == [nn.Conv2d, nn.Conv2d]):\n",
    "                    sorting_pairs.append(layer_list[idx:idx+2].insert(1,0))\n",
    "                    idx+=1\n",
    "                else:\n",
    "                    idx+=1\n",
    "            return sorting_pairs\n",
    "\n",
    "        detect_sequences(layer_types)\n",
    "\n",
    "        for keys, value in mapped_layers.items():\n",
    "            mapped_layers[keys] = np.asarray(mapped_layers[keys])\n",
    "\n",
    "        mapped_layers['name_type_shape'] = name_type_shape\n",
    "        mapped_layers['name_list'] = mapped_layers['name_type_shape'][:, 0]\n",
    "        mapped_layers['type_list'] = mapped_layers['name_type_shape'][:, 1]\n",
    "\n",
    "        # CWP\n",
    "        keys_to_lookout = ['Conv2d_BatchNorm2d_ReLU', 'Conv2d_BatchNorm2d']\n",
    "        pruning_layer_of_interest, qat_layer_of_interest = [], []\n",
    "\n",
    "        # CWP or QAT Fusion Layers\n",
    "        for keys in keys_to_lookout:\n",
    "            data = mapped_layers[keys]\n",
    "            if (len(data) != 0):\n",
    "                qat_layer_of_interest.append(data)\n",
    "        mapped_layers['qat_layers'] = qat_layer_of_interest\n",
    "        mapped_layers['model_summary'] = model_summary\n",
    "\n",
    "        name_list = mapped_layers['name_type_shape'][:, 0]\n",
    "        layer_name_list = []\n",
    "        w, x, y = [], [], []\n",
    "        for layer_name in name_list:\n",
    "            layer = eval(layer_name)\n",
    "            if(isinstance(layer,(nn.Conv2d,nn.Linear))):\n",
    "                layer_name_list.append(layer_name)\n",
    "                x.append(mapped_layers['model_summary'][layer_name]['x'][0])\n",
    "                w.append(mapped_layers['model_summary'][layer_name]['w'])\n",
    "                y.append(torch.stack(mapped_layers['model_summary'][layer_name]['y']))\n",
    "                # b.append(mapped_layers['model_summary'][layer_name]['b'])\n",
    "        \n",
    "        mapped_layers['catcher'] = {'name_list':layer_name_list, 'x':x,'w':w,'y':y}\n",
    "\n",
    "\n",
    "        mapped_layers['conv_list'] = [l_n for l_n in mapped_layers['catcher']['name_list'] if isinstance(eval(l_n), nn.Conv2d)]\n",
    "        mapped_layers['conv_bn_list'] = name_list[[index for index, layer in enumerate(mapped_layers['type_list']) if layer in ['Conv2d', 'BatchNorm2d']]]\n",
    "        mapped_layers['sorted_conv_list'] = detect_sorting_pairs(mapped_layers['conv_bn_list'])\n",
    "\n",
    "        return mapped_layers\n",
    "\n",
    "\n",
    "    # GMP\n",
    "    #         layer_of_interest=mapped_layers['name_type_shape'][:,0] # all layers with weights\n",
    "    #         Check for all with weights\n",
    "    # Wanda\n",
    "\n",
    "    def string_fixer(self,name_list):\n",
    "        for ind in range(len(name_list)):\n",
    "            modified_string = re.sub(r'\\.(\\[)', r'\\1', name_list[ind])\n",
    "            name_list[ind] = modified_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3f6f3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/beast/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/beast/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/beast/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# model = VGG().cuda()\n",
    "# checkpoint = torch.load('vgg.cifar.pretrained.pth')\n",
    "# model.load_state_dict(checkpoint)\n",
    "\n",
    "\n",
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "163a67f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.features[0][0] : Conv2d\n",
      "model.features[0][1] : BatchNorm2d\n",
      "model.features[0][2] : ReLU6\n",
      "model.features[1].conv[0][0] : Conv2d\n",
      "model.features[1].conv[0][1] : BatchNorm2d\n",
      "model.features[1].conv[0][2] : ReLU6\n",
      "model.features[1].conv[1] : Conv2d\n",
      "model.features[1].conv[2] : BatchNorm2d\n",
      "model.features[1] : InvertedResidual\n",
      "model.features[2].conv[0][0] : Conv2d\n",
      "model.features[2].conv[0][1] : BatchNorm2d\n",
      "model.features[2].conv[0][2] : ReLU6\n",
      "model.features[2].conv[1][0] : Conv2d\n",
      "model.features[2].conv[1][1] : BatchNorm2d\n",
      "model.features[2].conv[1][2] : ReLU6\n",
      "model.features[2].conv[2] : Conv2d\n",
      "model.features[2].conv[3] : BatchNorm2d\n",
      "model.features[2] : InvertedResidual\n",
      "model.features[3].conv[0][0] : Conv2d\n",
      "model.features[3].conv[0][1] : BatchNorm2d\n",
      "model.features[3].conv[0][2] : ReLU6\n",
      "model.features[3].conv[1][0] : Conv2d\n",
      "model.features[3].conv[1][1] : BatchNorm2d\n",
      "model.features[3].conv[1][2] : ReLU6\n",
      "model.features[3].conv[2] : Conv2d\n",
      "model.features[3].conv[3] : BatchNorm2d\n",
      "model.features[3] : InvertedResidual\n",
      "model.features[4].conv[0][0] : Conv2d\n",
      "model.features[4].conv[0][1] : BatchNorm2d\n",
      "model.features[4].conv[0][2] : ReLU6\n",
      "model.features[4].conv[1][0] : Conv2d\n",
      "model.features[4].conv[1][1] : BatchNorm2d\n",
      "model.features[4].conv[1][2] : ReLU6\n",
      "model.features[4].conv[2] : Conv2d\n",
      "model.features[4].conv[3] : BatchNorm2d\n",
      "model.features[4] : InvertedResidual\n",
      "model.features[5].conv[0][0] : Conv2d\n",
      "model.features[5].conv[0][1] : BatchNorm2d\n",
      "model.features[5].conv[0][2] : ReLU6\n",
      "model.features[5].conv[1][0] : Conv2d\n",
      "model.features[5].conv[1][1] : BatchNorm2d\n",
      "model.features[5].conv[1][2] : ReLU6\n",
      "model.features[5].conv[2] : Conv2d\n",
      "model.features[5].conv[3] : BatchNorm2d\n",
      "model.features[5] : InvertedResidual\n",
      "model.features[6].conv[0][0] : Conv2d\n",
      "model.features[6].conv[0][1] : BatchNorm2d\n",
      "model.features[6].conv[0][2] : ReLU6\n",
      "model.features[6].conv[1][0] : Conv2d\n",
      "model.features[6].conv[1][1] : BatchNorm2d\n",
      "model.features[6].conv[1][2] : ReLU6\n",
      "model.features[6].conv[2] : Conv2d\n",
      "model.features[6].conv[3] : BatchNorm2d\n",
      "model.features[6] : InvertedResidual\n",
      "model.features[7].conv[0][0] : Conv2d\n",
      "model.features[7].conv[0][1] : BatchNorm2d\n",
      "model.features[7].conv[0][2] : ReLU6\n",
      "model.features[7].conv[1][0] : Conv2d\n",
      "model.features[7].conv[1][1] : BatchNorm2d\n",
      "model.features[7].conv[1][2] : ReLU6\n",
      "model.features[7].conv[2] : Conv2d\n",
      "model.features[7].conv[3] : BatchNorm2d\n",
      "model.features[7] : InvertedResidual\n",
      "model.features[8].conv[0][0] : Conv2d\n",
      "model.features[8].conv[0][1] : BatchNorm2d\n",
      "model.features[8].conv[0][2] : ReLU6\n",
      "model.features[8].conv[1][0] : Conv2d\n",
      "model.features[8].conv[1][1] : BatchNorm2d\n",
      "model.features[8].conv[1][2] : ReLU6\n",
      "model.features[8].conv[2] : Conv2d\n",
      "model.features[8].conv[3] : BatchNorm2d\n",
      "model.features[8] : InvertedResidual\n",
      "model.features[9].conv[0][0] : Conv2d\n",
      "model.features[9].conv[0][1] : BatchNorm2d\n",
      "model.features[9].conv[0][2] : ReLU6\n",
      "model.features[9].conv[1][0] : Conv2d\n",
      "model.features[9].conv[1][1] : BatchNorm2d\n",
      "model.features[9].conv[1][2] : ReLU6\n",
      "model.features[9].conv[2] : Conv2d\n",
      "model.features[9].conv[3] : BatchNorm2d\n",
      "model.features[9] : InvertedResidual\n",
      "model.features[10].conv[0][0] : Conv2d\n",
      "model.features[10].conv[0][1] : BatchNorm2d\n",
      "model.features[10].conv[0][2] : ReLU6\n",
      "model.features[10].conv[1][0] : Conv2d\n",
      "model.features[10].conv[1][1] : BatchNorm2d\n",
      "model.features[10].conv[1][2] : ReLU6\n",
      "model.features[10].conv[2] : Conv2d\n",
      "model.features[10].conv[3] : BatchNorm2d\n",
      "model.features[10] : InvertedResidual\n",
      "model.features[11].conv[0][0] : Conv2d\n",
      "model.features[11].conv[0][1] : BatchNorm2d\n",
      "model.features[11].conv[0][2] : ReLU6\n",
      "model.features[11].conv[1][0] : Conv2d\n",
      "model.features[11].conv[1][1] : BatchNorm2d\n",
      "model.features[11].conv[1][2] : ReLU6\n",
      "model.features[11].conv[2] : Conv2d\n",
      "model.features[11].conv[3] : BatchNorm2d\n",
      "model.features[11] : InvertedResidual\n",
      "model.features[12].conv[0][0] : Conv2d\n",
      "model.features[12].conv[0][1] : BatchNorm2d\n",
      "model.features[12].conv[0][2] : ReLU6\n",
      "model.features[12].conv[1][0] : Conv2d\n",
      "model.features[12].conv[1][1] : BatchNorm2d\n",
      "model.features[12].conv[1][2] : ReLU6\n",
      "model.features[12].conv[2] : Conv2d\n",
      "model.features[12].conv[3] : BatchNorm2d\n",
      "model.features[12] : InvertedResidual\n",
      "model.features[13].conv[0][0] : Conv2d\n",
      "model.features[13].conv[0][1] : BatchNorm2d\n",
      "model.features[13].conv[0][2] : ReLU6\n",
      "model.features[13].conv[1][0] : Conv2d\n",
      "model.features[13].conv[1][1] : BatchNorm2d\n",
      "model.features[13].conv[1][2] : ReLU6\n",
      "model.features[13].conv[2] : Conv2d\n",
      "model.features[13].conv[3] : BatchNorm2d\n",
      "model.features[13] : InvertedResidual\n",
      "model.features[14].conv[0][0] : Conv2d\n",
      "model.features[14].conv[0][1] : BatchNorm2d\n",
      "model.features[14].conv[0][2] : ReLU6\n",
      "model.features[14].conv[1][0] : Conv2d\n",
      "model.features[14].conv[1][1] : BatchNorm2d\n",
      "model.features[14].conv[1][2] : ReLU6\n",
      "model.features[14].conv[2] : Conv2d\n",
      "model.features[14].conv[3] : BatchNorm2d\n",
      "model.features[14] : InvertedResidual\n",
      "model.features[15].conv[0][0] : Conv2d\n",
      "model.features[15].conv[0][1] : BatchNorm2d\n",
      "model.features[15].conv[0][2] : ReLU6\n",
      "model.features[15].conv[1][0] : Conv2d\n",
      "model.features[15].conv[1][1] : BatchNorm2d\n",
      "model.features[15].conv[1][2] : ReLU6\n",
      "model.features[15].conv[2] : Conv2d\n",
      "model.features[15].conv[3] : BatchNorm2d\n",
      "model.features[15] : InvertedResidual\n",
      "model.features[16].conv[0][0] : Conv2d\n",
      "model.features[16].conv[0][1] : BatchNorm2d\n",
      "model.features[16].conv[0][2] : ReLU6\n",
      "model.features[16].conv[1][0] : Conv2d\n",
      "model.features[16].conv[1][1] : BatchNorm2d\n",
      "model.features[16].conv[1][2] : ReLU6\n",
      "model.features[16].conv[2] : Conv2d\n",
      "model.features[16].conv[3] : BatchNorm2d\n",
      "model.features[16] : InvertedResidual\n",
      "model.features[17].conv[0][0] : Conv2d\n",
      "model.features[17].conv[0][1] : BatchNorm2d\n",
      "model.features[17].conv[0][2] : ReLU6\n",
      "model.features[17].conv[1][0] : Conv2d\n",
      "model.features[17].conv[1][1] : BatchNorm2d\n",
      "model.features[17].conv[1][2] : ReLU6\n",
      "model.features[17].conv[2] : Conv2d\n",
      "model.features[17].conv[3] : BatchNorm2d\n",
      "model.features[17] : InvertedResidual\n",
      "model.features[18][0] : Conv2d\n",
      "model.features[18][1] : BatchNorm2d\n",
      "model.features[18][2] : ReLU6\n",
      "model.classifier[0] : Dropout\n",
      "model.classifier[1] : Linear\n"
     ]
    }
   ],
   "source": [
    "ma = ModelAnalyzer(model)\n",
    "\n",
    "\n",
    "for layer_name, layer_type in zip(ma.mapped_layers['name_list'], ma.mapped_layers['type_list']):\n",
    "    print(f\"{layer_name} : {layer_type}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

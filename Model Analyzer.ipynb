{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a0da01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from collections import defaultdict, OrderedDict\n",
    "import ipdb\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab78cb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "image_size = 32\n",
    "transforms = {\n",
    "    \"train\": Compose([\n",
    "        RandomCrop(image_size, padding=4),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "    ]),\n",
    "    \"test\": ToTensor(),\n",
    "}\n",
    "dataset = {}\n",
    "for split in [\"train\", \"test\"]:\n",
    "  dataset[split] = CIFAR10(\n",
    "    root=\"data/cifar10\",\n",
    "    train=(split == \"train\"),\n",
    "    download=True,\n",
    "    transform=transforms[split],\n",
    "  )\n",
    "dataloader = {}\n",
    "for split in ['train', 'test']:\n",
    "  dataloader[split] = DataLoader(\n",
    "    dataset[split],\n",
    "    batch_size=512,\n",
    "    shuffle=(split == 'train'),\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "  )\n",
    "\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "  ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "  def __init__(self) -> None:\n",
    "      super().__init__()\n",
    "\n",
    "      layers = []\n",
    "      counts = defaultdict(int)\n",
    "\n",
    "      def add(name: str, layer: nn.Module) -> None:\n",
    "          layers.append((f\"{name}{counts[name]}\", layer))\n",
    "          counts[name] += 1\n",
    "\n",
    "      in_channels = 3\n",
    "      for x in self.ARCH:\n",
    "          if x != 'M':\n",
    "              # conv-bn-relu\n",
    "              add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n",
    "              add(\"bn\", nn.BatchNorm2d(x))\n",
    "              add(\"relu\", nn.ReLU(True))\n",
    "              in_channels = x\n",
    "          else:\n",
    "              # maxpool\n",
    "              add(\"pool\", nn.MaxPool2d(2))\n",
    "\n",
    "      self.backbone = nn.Sequential(OrderedDict(layers))\n",
    "      self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "      # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]\n",
    "      x = self.backbone(x)\n",
    "\n",
    "      # avgpool: [N, 512, 2, 2] => [N, 512]\n",
    "      x = x.mean([2, 3])\n",
    "\n",
    "      # classifier: [N, 512] => [N, 10]\n",
    "      x = self.classifier(x)\n",
    "      return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e458a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class ModelAnalyzer:\n",
    "    def __init__(self, model, dataloader):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.mapped_layers = self.layer_mapping()\n",
    "        \n",
    "\n",
    "    \n",
    "    def name_fixer(self, names):\n",
    "       \n",
    "        return_list = []\n",
    "        for string in names:\n",
    "            matches = re.finditer(r'\\.\\[(\\d+)\\]', string)\n",
    "            pop_list = [m.start(0) for m in matches]\n",
    "            pop_list.sort(reverse=True)\n",
    "            if len(pop_list) > 0:\n",
    "                string = list(string)\n",
    "                for pop_id in pop_list:\n",
    "                    string.pop(pop_id)\n",
    "                string = ''.join(string)\n",
    "            return_list.append(string)\n",
    "        return return_list\n",
    "\n",
    "\n",
    "    def get_all_layers(self, model, parent_name=''):\n",
    "        layers = []\n",
    "        for name, module in model.named_children():\n",
    "            full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "            test_name = \"model.\" + full_name\n",
    "            try:\n",
    "                eval(test_name)\n",
    "                layers.append((full_name, module))\n",
    "            except:\n",
    "                layers.append((self.reformat_layer_name(full_name), module))\n",
    "            if isinstance(module, nn.Module):\n",
    "                layers.extend(self.get_all_layers(module, parent_name=full_name))\n",
    "        return layers\n",
    "\n",
    "\n",
    "    def reformat_layer_name(self, str_data):\n",
    "        try:\n",
    "            split_data = str_data.split('.')\n",
    "            for ind in range(len(split_data)):\n",
    "                data = split_data[ind]\n",
    "                if (data.isdigit()):\n",
    "                    split_data[ind] = \"[\" + data + \"]\"\n",
    "            final_string = '.'.join(split_data)\n",
    "\n",
    "            iters_a = re.finditer(r'[a-zA-Z]\\.\\[', final_string)\n",
    "            indices = [m.start(0) + 1 for m in iters_a]\n",
    "            iters = re.finditer(r'\\]\\.\\[', final_string)\n",
    "            indices.extend([m.start(0) + 1 for m in iters])\n",
    "\n",
    "            final_string = list(final_string)\n",
    "            final_string = [final_string[i] for i in range(len(final_string)) if i not in indices]\n",
    "\n",
    "            str_data = ''.join(final_string)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return str_data\n",
    "\n",
    "\n",
    "    def summary_string_fixed(self, model, all_layers, input_size, model_name=None, batch_size=-1, dtypes=None):\n",
    "        if dtypes is None:\n",
    "            dtypes = [torch.FloatTensor] * len(input_size)\n",
    "\n",
    "        def register_hook(module, module_idx):\n",
    "            def hook(module, input, output):\n",
    "                nonlocal module_idx\n",
    "                m_key = all_layers[module_idx][0]\n",
    "                m_key = model_name + \".\" + m_key\n",
    "\n",
    "                try:\n",
    "                    eval(m_key)\n",
    "                except:\n",
    "                    m_key = name_fixer([m_key])[0]\n",
    "\n",
    "                summary[m_key] = OrderedDict()\n",
    "                summary[m_key][\"type\"] = str(type(module)).split('.')[-1][:-2]\n",
    "                summary[m_key][\"x\"] = input\n",
    "                summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "                summary[m_key][\"input_shape\"][0] = batch_size\n",
    "\n",
    "                if isinstance(output, (list, tuple)):\n",
    "                    summary[m_key][\"y\"] = [\n",
    "                        [-1] + list(o)[1:] for o in output\n",
    "                    ]\n",
    "                    summary[m_key][\"output_shape\"] = [\n",
    "                        [-1] + list(o.size())[1:] for o in output\n",
    "                    ]\n",
    "                else:\n",
    "                    summary[m_key][\"y\"] = list(output)\n",
    "\n",
    "\n",
    "                    summary[m_key][\"output_shape\"] = list(output.size())\n",
    "                    summary[m_key][\"output_shape\"][0] = batch_size\n",
    "\n",
    "                params = 0\n",
    "                if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                    summary[m_key][\"w\"] = module.weight\n",
    "                    params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "                    summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
    "                    summary[m_key][\"weight_shape\"] = module.weight.shape\n",
    "                if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                    summary[m_key][\"b\"] = module.bias\n",
    "                    params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "                summary[m_key][\"nb_params\"] = params\n",
    "\n",
    "            if (\n",
    "                    not isinstance(module, nn.Sequential)\n",
    "                    and not isinstance(module, nn.ModuleList)\n",
    "            ):\n",
    "                hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "        if isinstance(input_size, tuple):\n",
    "            input_size = [input_size]\n",
    "            \n",
    "        model_device = next(iter(model.parameters())).device\n",
    "        x, _ = next(iter(self.dataloader['test']))\n",
    "        x = x.to(model_device)\n",
    "\n",
    "        summary = OrderedDict()\n",
    "        hooks = []\n",
    "\n",
    "        for module_idx, (layer_name, module) in enumerate(all_layers):\n",
    "            register_hook(module, module_idx)\n",
    "\n",
    "        model(x)\n",
    "\n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "\n",
    "        return summary\n",
    "\n",
    "\n",
    "    def layer_mapping(self):\n",
    "        all_layers = self.get_all_layers(self.model)\n",
    "        x,y = next(iter(self.dataloader['test']))\n",
    "        model_summary = self.summary_string_fixed(model, all_layers, x.shape, model_name='model')\n",
    "\n",
    "        name_type_shape = []\n",
    "        for key in model_summary.keys():\n",
    "            data = model_summary[key]\n",
    "            if (\"weight_shape\" in data.keys()):\n",
    "                name_type_shape.append([key, data['type'], data['weight_shape'][0]])\n",
    "            else:\n",
    "                name_type_shape.append([key, data['type'], 0 ])\n",
    "        name_type_shape = np.asarray(name_type_shape)\n",
    "\n",
    "        name_list = name_type_shape[:, 0]\n",
    "\n",
    "        r_name_list = np.asarray(name_list)\n",
    "        random_picks = np.random.randint(0, len(r_name_list), 10)\n",
    "        test_name_list = r_name_list[random_picks]\n",
    "        eval_hit = False\n",
    "        for layer in test_name_list:\n",
    "            try:\n",
    "                eval(layer)\n",
    "\n",
    "            except:\n",
    "                eval_hit = True\n",
    "                break\n",
    "        if (eval_hit):\n",
    "            fixed_name_list = name_fixer(r_name_list)\n",
    "            name_type_shape[:, 0] = fixed_name_list\n",
    "\n",
    "        layer_types = name_type_shape[:, 1]\n",
    "        layer_shapes = name_type_shape[:, 2]\n",
    "        fusion_layers = {'Conv2d_BatchNorm2d_ReLU': [], 'Conv2d_BatchNorm2d': [], 'Linear_ReLU': [],\n",
    "                        'Linear_BatchNorm1d': []}\n",
    "\n",
    "        def detect_sequences(lst):\n",
    "            fusing_layers = [\n",
    "                            'Conv2d',\n",
    "                            'BatchNorm2d',\n",
    "                            'ReLU',\n",
    "                            'Linear',\n",
    "                            'BatchNorm1d',\n",
    "                        ]\n",
    "\n",
    "            i = 0\n",
    "            while i < len(lst):\n",
    "\n",
    "                if i + 2 < len(lst) and [l for l in lst[i: i + 3]] == [\n",
    "                    fusing_layers[0],\n",
    "                    fusing_layers[1],\n",
    "                    fusing_layers[2],\n",
    "                ]:\n",
    "                    \n",
    "                    fusion_layers['Conv2d_BatchNorm2d_ReLU'].append(\n",
    "                            np.take(name_list, [i for i in range(i, i + 3)]).tolist()\n",
    "                        )\n",
    "                    i += 3\n",
    "\n",
    "                elif i + 1 < len(lst) and [l for l in lst[i: i + 2]] == [\n",
    "                    fusing_layers[0],\n",
    "                    fusing_layers[1],\n",
    "                ]:\n",
    "\n",
    "                    fusion_layers['Conv2d_BatchNorm2d'].append(\n",
    "                            np.take(name_list, [i for i in range(i, i + 2)]).tolist()\n",
    "                        )\n",
    "                    i += 2\n",
    "\n",
    "                elif i + 1 < len(lst) and [l for l in lst[i: i + 2]] == [\n",
    "                    fusing_layers[3],\n",
    "                    fusing_layers[2],\n",
    "                ]:\n",
    "                    fusion_layers['Linear_ReLU'].append(\n",
    "                        np.take(name_list, [i for i in range(i, i + 2)]).tolist()\n",
    "                    )\n",
    "                    i += 2\n",
    "                elif i + 1 < len(lst) and [l for l in lst[i: i + 2]] == [\n",
    "                    fusing_layers[3],\n",
    "                    fusing_layers[4],\n",
    "                ]:\n",
    "                    fusion_layers['Linear_BatchNorm1d'].append(\n",
    "                        np.take(name_list, [i for i in range(i, i + 2)]).tolist()\n",
    "                    )\n",
    "                    i += 2\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "        def detect_sorting_pairs(name_list):\n",
    "            sorting_pairs = []\n",
    "            idx = 0\n",
    "            while idx < len(name_list):\n",
    "                layer_list = [ layer_name for layer_name in name_list[idx:idx+3]]\n",
    "                \n",
    "                if([ type(eval(l)) for l in layer_list] == [nn.Conv2d, nn.BatchNorm2d,nn.Conv2d]):\n",
    "                    sorting_pairs.append(layer_list)\n",
    "                    idx+=2\n",
    "                elif([ type(eval(l)) for l in layer_list[idx:idx+2]] == [nn.Conv2d, nn.Conv2d]):\n",
    "                    sorting_pairs.append(layer_list[idx:idx+2].insert(1,0))\n",
    "                    idx+=1\n",
    "                else:\n",
    "                    idx+=1\n",
    "            return sorting_pairs\n",
    "\n",
    "        detect_sequences(layer_types)\n",
    "        mapped_layers = {}\n",
    "        # for keys, value in fusion_layers.items():\n",
    "        #     mapped_layers[keys] = np.asarray(fusion_layers[keys])\n",
    "\n",
    "        mapped_layers['name_type_shape'] = name_type_shape\n",
    "        mapped_layers['name_list'] = mapped_layers['name_type_shape'][:, 0]\n",
    "        mapped_layers['type_list'] = mapped_layers['name_type_shape'][:, 1]\n",
    "\n",
    "        # CWP\n",
    "        keys_to_lookout = ['Conv2d_BatchNorm2d_ReLU', 'Conv2d_BatchNorm2d']\n",
    "        pruning_layer_of_interest, qat_layer_of_interest = [], []\n",
    "\n",
    "        # CWP or QAT Fusion Layers\n",
    "        # for keys in keys_to_lookout:\n",
    "        #     data = mapped_layers[keys]\n",
    "        #     if (len(data) != 0):\n",
    "        #         qat_layer_of_interest.append(data)\n",
    "        mapped_layers['qat_layers'] = qat_layer_of_interest\n",
    "        mapped_layers['model_summary'] = model_summary\n",
    "\n",
    "        name_list = mapped_layers['name_type_shape'][:, 0]\n",
    "        layer_name_list = []\n",
    "        w, x, y = [], [], []\n",
    "        for layer_name in name_list:\n",
    "            layer = eval(layer_name)\n",
    "            if(isinstance(layer,(nn.Conv2d,nn.Linear))):\n",
    "                layer_name_list.append(layer_name)\n",
    "                x.append(mapped_layers['model_summary'][layer_name]['x'][0])\n",
    "                w.append(mapped_layers['model_summary'][layer_name]['w'])\n",
    "                y.append(torch.stack(mapped_layers['model_summary'][layer_name]['y']))\n",
    "                # b.append(mapped_layers['model_summary'][layer_name]['b'])\n",
    "        \n",
    "        mapped_layers['catcher'] = {'name_list':layer_name_list, 'x':x,'w':w,'y':y}\n",
    "\n",
    "\n",
    "        mapped_layers['conv_list'] = [l_n for l_n in mapped_layers['catcher']['name_list'] if isinstance(eval(l_n), nn.Conv2d)]\n",
    "        mapped_layers['conv_bn_list'] = name_list[[index for index, layer in enumerate(mapped_layers['type_list']) if layer in ['Conv2d', 'BatchNorm2d']]]\n",
    "        mapped_layers['sorted_conv_list'] = detect_sorting_pairs(mapped_layers['conv_bn_list'])\n",
    "        mapped_layers['fusion_layers'] = fusion_layers\n",
    "\n",
    "        return mapped_layers\n",
    "\n",
    "    # GMP\n",
    "    #         layer_of_interest=mapped_layers['name_type_shape'][:,0] # all layers with weights\n",
    "    #         Check for all with weights\n",
    "    # Wanda\n",
    "\n",
    "    def string_fixer(self,name_list):\n",
    "        for ind in range(len(name_list)):\n",
    "            modified_string = re.sub(r'\\.(\\[)', r'\\1', name_list[ind])\n",
    "            name_list[ind] = modified_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3f6f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG()\n",
    "checkpoint = torch.load('vgg.cifar.pretrained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "163a67f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = ModelAnalyzer(model, dataloader)\n",
    "mapped_layers = ma.mapped_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5dce06de-f7e6-4912-bd76-08de893b9bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name_type_shape', 'name_list', 'type_list', 'qat_layers', 'model_summary', 'catcher', 'conv_list', 'conv_bn_list', 'sorted_conv_list', 'fusion_layers'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_layers.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d55e2761-eaf9-4694-b04b-0d9c856c3bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Conv2d_BatchNorm2d_ReLU': [['model.backbone.conv0',\n",
       "   'model.backbone.bn0',\n",
       "   'model.backbone.relu0'],\n",
       "  ['model.backbone.conv1', 'model.backbone.bn1', 'model.backbone.relu1'],\n",
       "  ['model.backbone.conv2', 'model.backbone.bn2', 'model.backbone.relu2'],\n",
       "  ['model.backbone.conv3', 'model.backbone.bn3', 'model.backbone.relu3'],\n",
       "  ['model.backbone.conv4', 'model.backbone.bn4', 'model.backbone.relu4'],\n",
       "  ['model.backbone.conv5', 'model.backbone.bn5', 'model.backbone.relu5'],\n",
       "  ['model.backbone.conv6', 'model.backbone.bn6', 'model.backbone.relu6'],\n",
       "  ['model.backbone.conv7', 'model.backbone.bn7', 'model.backbone.relu7']],\n",
       " 'Conv2d_BatchNorm2d': [],\n",
       " 'Linear_ReLU': [],\n",
       " 'Linear_BatchNorm1d': []}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_layers['fusion_layers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16c643e2-ab09-434e-b2d5-89c437a8724f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.backbone.conv0 : Conv2d\n",
      "model.backbone.bn0 : BatchNorm2d\n",
      "model.backbone.relu0 : ReLU\n",
      "model.backbone.conv1 : Conv2d\n",
      "model.backbone.bn1 : BatchNorm2d\n",
      "model.backbone.relu1 : ReLU\n",
      "model.backbone.pool0 : MaxPool2d\n",
      "model.backbone.conv2 : Conv2d\n",
      "model.backbone.bn2 : BatchNorm2d\n",
      "model.backbone.relu2 : ReLU\n",
      "model.backbone.conv3 : Conv2d\n",
      "model.backbone.bn3 : BatchNorm2d\n",
      "model.backbone.relu3 : ReLU\n",
      "model.backbone.pool1 : MaxPool2d\n",
      "model.backbone.conv4 : Conv2d\n",
      "model.backbone.bn4 : BatchNorm2d\n",
      "model.backbone.relu4 : ReLU\n",
      "model.backbone.conv5 : Conv2d\n",
      "model.backbone.bn5 : BatchNorm2d\n",
      "model.backbone.relu5 : ReLU\n",
      "model.backbone.pool2 : MaxPool2d\n",
      "model.backbone.conv6 : Conv2d\n",
      "model.backbone.bn6 : BatchNorm2d\n",
      "model.backbone.relu6 : ReLU\n",
      "model.backbone.conv7 : Conv2d\n",
      "model.backbone.bn7 : BatchNorm2d\n",
      "model.backbone.relu7 : ReLU\n",
      "model.backbone.pool3 : MaxPool2d\n",
      "model.classifier : Linear\n"
     ]
    }
   ],
   "source": [
    "for layer_name, layer_type in zip(mapped_layers['name_list'], mapped_layers['type_list']):\n",
    "    print(f\"{layer_name} : {layer_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "322336fd-1669-46f4-b69c-ac777659c6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name: model.backbone.conv0 I/O Shape: torch.Size([512, 3, 32, 32]) Weight Shape: torch.Size([64, 3, 3, 3]) O/P Shape: torch.Size([512, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for name, x,w,y in zip(mapped_layers['catcher']['name_list'],mapped_layers['catcher']['x'],mapped_layers['catcher']['w'],mapped_layers['catcher']['y']):\n",
    "    print(f\"Layer Name: {name} I/O Shape: {x.shape} Weight Shape: {w.shape} O/P Shape: {x.shape}\")\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

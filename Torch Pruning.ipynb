{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "893587df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "import torch.optim as optim\n",
    "import re\n",
    "from torchsummary import summary\n",
    "\n",
    "import torch_pruning as tp\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1b5f528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/sathya/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/Users/sathya/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sathya/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "class VGG(nn.Module):\n",
    "    ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        counts = defaultdict(int)\n",
    "\n",
    "        def add(name: str, layer: nn.Module) -> None:\n",
    "            layers.append((f\"{name}{counts[name]}\", layer))\n",
    "            counts[name] += 1\n",
    "\n",
    "        in_channels = 3\n",
    "        for x in self.ARCH:\n",
    "            if x != 'M':\n",
    "                # conv-bn-relu\n",
    "                add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n",
    "                add(\"bn\", nn.BatchNorm2d(x))\n",
    "                add(\"relu\", nn.ReLU(True))\n",
    "                in_channels = x\n",
    "            else:\n",
    "                # maxpool\n",
    "                add(\"pool\", nn.MaxPool2d(2))\n",
    "\n",
    "        self.backbone = nn.Sequential(OrderedDict(layers))\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        # avgpool: [N, 512, 2, 2] => [N, 512]\n",
    "        x = x.mean([2, 3])\n",
    "\n",
    "        # classifier: [N, 512] => [N, 10]\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "vgg = VGG()\n",
    "mobilenet_v3 = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v3_small', pretrained=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1208e192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "image_size = 32\n",
    "transforms = {\n",
    "    \"train\": Compose([\n",
    "        RandomCrop(image_size, padding=4),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "    ]),\n",
    "    \"test\": ToTensor(),\n",
    "}\n",
    "dataset = {}\n",
    "for split in [\"train\", \"test\"]:\n",
    "  dataset[split] = CIFAR10(\n",
    "    root=\"data/cifar10\",\n",
    "    train=(split == \"train\"),\n",
    "    download=True,\n",
    "    transform=transforms[split],\n",
    "  )\n",
    "dataloader = {}\n",
    "for split in ['train', 'test']:\n",
    "  dataloader[split] = DataLoader(\n",
    "    dataset[split],\n",
    "    batch_size=512,\n",
    "    shuffle=(split == 'train'),\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "  )\n",
    "test_input = next(iter(dataloader['test']))[0][:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b5e1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8293e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Pruning\n",
    "# sparsity_dict = {model.backbone.conv0: 0.15000000000000002, model.backbone.conv1: 0.15, model.backbone.conv2: 0.15, model.backbone.conv3: 0.15000000000000002, model.backbone.conv4: 0.20000000000000004, model.backbone.conv5: 0.20000000000000004, model.backbone.conv6: 0.45000000000000007}\n",
    "# imp = tp.importance.MagnitudeImportance(p=1, group_reduction='mean')\n",
    "\n",
    "# pruner = tp.pruner.MetaPruner( \n",
    "#     model,\n",
    "#     test_input,\n",
    "#     importance=imp,\n",
    "#     pruning_ratio_dict = sparsity_dict, \n",
    "# #     ignored_layers=ignored_layers,\n",
    "# )\n",
    "\n",
    "# pruner.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edaad4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.zero_grad()\n",
    "# summary(model,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b40f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c184b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_layer_name(str_data):\n",
    "\n",
    "    split_data = str_data.split('.')\n",
    "\n",
    "    for ind in range(len(split_data)):\n",
    "        data = split_data[ind]\n",
    "        if (data.isdigit()):\n",
    "            split_data[ind] = \"[\" + data + \"]\"\n",
    "    final_string = '.'.join(split_data)\n",
    "\n",
    "    iters_a = re.finditer(r'[a-zA-Z]\\.\\[', final_string)\n",
    "    indices = [m.start(0) + 1 for m in iters_a]\n",
    "    iters = re.finditer(r'\\]\\.\\[', final_string)\n",
    "    indices.extend([m.start(0) + 1 for m in iters])\n",
    "\n",
    "    final_string = list(final_string)\n",
    "    final_string = [final_string[i] for i in range(len(final_string)) if i not in indices]\n",
    "\n",
    "    str_data = ''.join(final_string)\n",
    "\n",
    "    \n",
    "\n",
    "    return str_data\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_random_array(n):\n",
    "    random_array = []\n",
    "    \n",
    "    # Generate a random number of values between 0 and n\n",
    "    num_values = random.randint(0, n)\n",
    "    \n",
    "    # Generate the random array with unique values\n",
    "    row = random.sample(range(0, n+1), num_values)\n",
    "    random_array.append(row)\n",
    "    \n",
    "    return random_array[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78f5d122",
   "metadata": {},
   "outputs": [],
   "source": [
    "fusing_layers = [\n",
    "    'Conv2d',\n",
    "    'BatchNorm2d',\n",
    "    'ReLU',\n",
    "    'Linear',\n",
    "    'BatchNorm1d',\n",
    "]\n",
    "\n",
    "\n",
    "def summary_string_fixed(model, all_layers, input_size, model_name=None, batch_size=-1, dtypes=None):\n",
    "    if dtypes is None:\n",
    "        dtypes = [torch.FloatTensor] * len(input_size)\n",
    "        \n",
    "    def name_fixer(names):\n",
    "\n",
    "        return_list = []\n",
    "        for string in names:\n",
    "            matches = re.finditer(r'\\.\\[(\\d+)\\]', string)\n",
    "            pop_list = [m.start(0) for m in matches]\n",
    "            pop_list.sort(reverse=True)\n",
    "            if len(pop_list) > 0:\n",
    "                string = list(string)\n",
    "                for pop_id in pop_list:\n",
    "                    string.pop(pop_id)\n",
    "                string = ''.join(string)\n",
    "            return_list.append(string)\n",
    "        return return_list\n",
    "\n",
    "    def register_hook(module, module_idx):\n",
    "        def hook(module, input, output):\n",
    "            nonlocal module_idx\n",
    "            m_key = all_layers[module_idx][0]\n",
    "            m_key = model_name + \".\" + m_key\n",
    "\n",
    "            try:\n",
    "                eval(m_key)\n",
    "            except:\n",
    "                m_key = name_fixer([m_key])[0]\n",
    "\n",
    "            summary[m_key] = OrderedDict()\n",
    "            summary[m_key][\"type\"] = str(type(module)).split('.')[-1][:-2]\n",
    "            summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "            summary[m_key][\"input_shape\"][0] = batch_size\n",
    "\n",
    "            if isinstance(output, (list, tuple)):\n",
    "                summary[m_key][\"output_shape\"] = [\n",
    "                    [-1] + list(o.size())[1:] for o in output\n",
    "                ]\n",
    "            else:\n",
    "                summary[m_key][\"output_shape\"] = list(output.size())\n",
    "                summary[m_key][\"output_shape\"][0] = batch_size\n",
    "\n",
    "            params = 0\n",
    "            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "                summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
    "                summary[m_key][\"weight_shape\"] = module.weight.shape\n",
    "            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "            summary[m_key][\"nb_params\"] = params\n",
    "\n",
    "        if (\n",
    "                not isinstance(module, nn.Sequential)\n",
    "                and not isinstance(module, nn.ModuleList)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "    if isinstance(input_size, tuple):\n",
    "        input_size = [input_size]\n",
    "\n",
    "    x = [torch.rand(2, *in_size).type(dtype)\n",
    "         for in_size, dtype in zip(input_size, dtypes)]\n",
    "\n",
    "    summary = OrderedDict()\n",
    "    hooks = []\n",
    "\n",
    "    for module_idx, (layer_name, module) in enumerate(all_layers):\n",
    "        register_hook(module, module_idx)\n",
    "\n",
    "    model(*x)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return summary\n",
    "\n",
    "def layer_mapping(model):\n",
    "    \n",
    "    def get_all_layers(model, parent_name=''):\n",
    "        layers = []\n",
    "\n",
    "        def reformat_layer_name(str_data):\n",
    "            try:\n",
    "                split_data = str_data.split('.')\n",
    "                for ind in range(len(split_data)):\n",
    "                    data = split_data[ind]\n",
    "                    if (data.isdigit()):\n",
    "                        split_data[ind] = \"[\" + data + \"]\"\n",
    "                final_string = '.'.join(split_data)\n",
    "\n",
    "                iters_a = re.finditer(r'[a-zA-Z]\\.\\[', final_string)\n",
    "                indices = [m.start(0) + 1 for m in iters_a]\n",
    "                iters = re.finditer(r'\\]\\.\\[', final_string)\n",
    "                indices.extend([m.start(0) + 1 for m in iters])\n",
    "\n",
    "                final_string = list(final_string)\n",
    "                final_string = [final_string[i] for i in range(len(final_string)) if i not in indices]\n",
    "\n",
    "                str_data = ''.join(final_string)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            return str_data\n",
    "\n",
    "        for name, module in model.named_children():\n",
    "            full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "            test_name = \"model.\" + full_name\n",
    "            try:\n",
    "                eval(test_name)\n",
    "                layers.append((full_name, module))\n",
    "            except:\n",
    "                layers.append((reformat_layer_name(full_name), module))\n",
    "            if isinstance(module, nn.Module):\n",
    "                layers.extend(get_all_layers(module, parent_name=full_name))\n",
    "        return layers\n",
    "    all_layers = get_all_layers(model)\n",
    "    model_summary = summary_string_fixed(model, all_layers, (3, 64, 64), model_name='model')  # , device=\"cuda\")\n",
    "\n",
    "    name_type_shape = []\n",
    "    for key in model_summary.keys():\n",
    "        data = model_summary[key]\n",
    "        if (\"weight_shape\" in data.keys()):\n",
    "            name_type_shape.append([key, data['type'], data['weight_shape'][0]])\n",
    "        #     else:\n",
    "    #         name_type_shape.append([key, data['type'], 0 ])\n",
    "    name_type_shape = np.asarray(name_type_shape)\n",
    "\n",
    "    name_list = name_type_shape[:, 0]\n",
    "\n",
    "    r_name_list = np.asarray(name_list)\n",
    "    random_picks = np.random.randint(0, len(r_name_list), 10)\n",
    "    test_name_list = r_name_list[random_picks]\n",
    "    eval_hit = False\n",
    "    for layer in test_name_list:\n",
    "        try:\n",
    "            eval(layer)\n",
    "\n",
    "        except:\n",
    "            eval_hit = True\n",
    "            break\n",
    "    if (eval_hit):\n",
    "        fixed_name_list = name_fixer(r_name_list)\n",
    "        name_type_shape[:, 0] = fixed_name_list\n",
    "\n",
    "    layer_types = name_type_shape[:, 1]\n",
    "    layer_shapes = name_type_shape[:, 2]\n",
    "    mapped_layers = {'model_layer': [], 'Conv2d_BatchNorm2d_ReLU': [], 'Conv2d_BatchNorm2d': [], 'Linear_ReLU': [],\n",
    "                     'Linear_BatchNorm1d': []}\n",
    "\n",
    "    def detect_sequences(lst):\n",
    "        i = 0\n",
    "        while i < len(lst):\n",
    "\n",
    "            if i + 2 < len(lst) and [l for l in lst[i: i + 3]] == [\n",
    "                fusing_layers[0],\n",
    "                fusing_layers[1],\n",
    "                fusing_layers[2],\n",
    "            ]:\n",
    "                test_layer = layer_shapes[i: i + 2]\n",
    "                if (np.all(test_layer == test_layer[0])):\n",
    "                    mapped_layers['Conv2d_BatchNorm2d_ReLU'].append(\n",
    "                        np.take(name_list, [i for i in range(i, i + 3)]).tolist()\n",
    "                    )\n",
    "                    i += 3\n",
    "\n",
    "            elif i + 1 < len(lst) and [l for l in lst[i: i + 2]] == [\n",
    "                fusing_layers[0],\n",
    "                fusing_layers[1],\n",
    "            ]:\n",
    "                test_layer = layer_shapes[i: i + 2]\n",
    "                if (np.all(test_layer == test_layer[0])):\n",
    "                    mapped_layers['Conv2d_BatchNorm2d'].append(\n",
    "                        np.take(name_list, [i for i in range(i, i + 2)]).tolist()\n",
    "                    )\n",
    "                    i += 2\n",
    "            # if i + 1 < len(lst) and [ type(l) for l in lst[i:i+2]] == [fusing_layers[0], fusing_layers[2]]:\n",
    "            #     detected_sequences.append(np.take(name_list,[i for i in range(i,i+2)]).tolist())\n",
    "            #     i += 2\n",
    "            # elif i + 1 < len(lst) and [ type(l) for l in lst[i:i+2]] == [fusing_layers[1], fusing_layers[2]]:\n",
    "            #     detected_sequences.append(np.take(name_list,[i for i in range(i,i+2)]).tolist())\n",
    "            #     i += 2\n",
    "            elif i + 1 < len(lst) and [l for l in lst[i: i + 2]] == [\n",
    "                fusing_layers[3],\n",
    "                fusing_layers[2],\n",
    "            ]:\n",
    "                mapped_layers['Linear_ReLU'].append(\n",
    "                    np.take(name_list, [i for i in range(i, i + 2)]).tolist()\n",
    "                )\n",
    "                i += 2\n",
    "            elif i + 1 < len(lst) and [l for l in lst[i: i + 2]] == [\n",
    "                fusing_layers[3],\n",
    "                fusing_layers[4],\n",
    "            ]:\n",
    "                mapped_layers['Linear_BatchNorm1d'].append(\n",
    "                    np.take(name_list, [i for i in range(i, i + 2)]).tolist()\n",
    "                )\n",
    "                i += 2\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    detect_sequences(layer_types)\n",
    "\n",
    "    for keys, value in mapped_layers.items():\n",
    "        mapped_layers[keys] = np.asarray(mapped_layers[keys])\n",
    "\n",
    "    mapped_layers['name_type_shape'] = name_type_shape\n",
    "    # self.mapped_layers = mapped_layers\n",
    "\n",
    "    # CWP\n",
    "    keys_to_lookout = ['Conv2d_BatchNorm2d_ReLU', 'Conv2d_BatchNorm2d']\n",
    "    pruning_layer_of_interest, qat_layer_of_interest = [], []\n",
    "\n",
    "    # CWP or QAT Fusion Layers\n",
    "    for keys in keys_to_lookout:\n",
    "        data = mapped_layers[keys]\n",
    "        if (len(data) != 0):\n",
    "            qat_layer_of_interest.append(data)\n",
    "    mapped_layers['qat_layers'] = np.asarray(qat_layer_of_interest)\n",
    "\n",
    "    return mapped_layers\n",
    "\n",
    "def cwp_possible_layers(layer_name_list):\n",
    "    possible_indices = []\n",
    "    # for idx in range(len(layer_shape_list)):\n",
    "    idx = 0\n",
    "    while idx < len(layer_name_list):\n",
    "        \n",
    "        current_value = layer_name_list[idx]\n",
    "        layer_shape = eval(current_value).weight.shape\n",
    "        curr_merge_list = []\n",
    "        curr_merge_list.append([current_value, 0])\n",
    "        hit_catch = False\n",
    "        for internal_idx in range(idx + 1, len(layer_name_list) - 1):\n",
    "\n",
    "\n",
    "            new_layer = layer_name_list[internal_idx]\n",
    "            new_layer_shape = eval(new_layer).weight.shape\n",
    "\n",
    "            if (len(new_layer_shape) == 4):\n",
    "                curr_merge_list.append([new_layer, 0])\n",
    "            if (layer_shape[0] == new_layer_shape[1]):\n",
    "                hit_catch = True\n",
    "                break\n",
    "\n",
    "            elif (len(new_layer_shape) == 1):\n",
    "            #                 ipdb.set_trace()\n",
    "                curr_merge_list[len(curr_merge_list) - 1][1] = new_layer\n",
    "\n",
    "        possible_indices.append(curr_merge_list)\n",
    "        if (hit_catch == True):\n",
    "\n",
    "            idx = internal_idx\n",
    "        else:\n",
    "            idx += 1\n",
    "\n",
    "    return possible_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff518e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "# mobilenet_v3 = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v3_small', pretrained=True)\n",
    "# mobilenet_v3 = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "\n",
    "pruned_model  = copy.deepcopy(VGG())\n",
    "\n",
    "\n",
    "mapped_layers = layer_mapping(pruned_model)\n",
    "name_list = mapped_layers['name_type_shape'][:, 0]\n",
    "conv_list = name_list[mapped_layers['name_type_shape'][:, 1]=='Conv2d']\n",
    "total_convs = len(conv_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c28fc0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ipbd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m next_bn, next_convs \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     25\u001b[0m first_bn_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m ipbd\u001b[38;5;241m.\u001b[39mset_trace()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_lid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(group)):\n\u001b[1;32m     28\u001b[0m     layer \u001b[38;5;241m=\u001b[39m group[n_lid]\u001b[38;5;241m.\u001b[39mdep\u001b[38;5;241m.\u001b[39mtarget\u001b[38;5;241m.\u001b[39mmodule\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ipbd' is not defined"
     ]
    }
   ],
   "source": [
    "import ipdb\n",
    "DG = tp.DependencyGraph().build_dependency(pruned_model, example_inputs=test_input)\n",
    "ignored_layers = []\n",
    "\n",
    "for m in pruned_model.modules():\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        ignored_layers.append(m) # DO NOT prune the final classifier!\n",
    "\n",
    "ignore_last_conv=True\n",
    "\n",
    "for group in DG.get_all_groups(ignored_layers=ignored_layers, root_module_types=[nn.Conv2d]):\n",
    "    if(ignore_last_conv):\n",
    "        ignore_last_conv=False\n",
    "        continue\n",
    "    else:\n",
    "        \n",
    "        for i, (dep, idxs) in enumerate(group):\n",
    "            out_layer = dep.layer\n",
    "            \n",
    "            if(isinstance(out_layer, torch.nn.Conv2d)):\n",
    "                \n",
    "                prev_conv = out_layer\n",
    "                root_idxs = group[i].root_idxs\n",
    "                next_bn, next_convs = [], []\n",
    "                first_bn_flag = True\n",
    "                \n",
    "                for n_lid in range(len(group)):\n",
    "                    layer = group[n_lid].dep.target.module\n",
    "\n",
    "                    if isinstance(layer, torch.nn.BatchNorm2d):\n",
    "                        print(first_bn_flag)\n",
    "                        if first_bn_flag:\n",
    "                            prev_bn = layer\n",
    "                            first_bn_flag = False\n",
    "                        else:\n",
    "                            next_bn.append(layer)\n",
    "                    elif isinstance(layer, torch.nn.Conv2d):\n",
    "                        next_convs.append(layer)\n",
    "\n",
    "\n",
    "                print(\"Prev:\",prev_conv,prev_bn)\n",
    "                print(\"Next:\",next_convs,next_bn)\n",
    "                print(\"++++++++++++++++++++++++++\")\n",
    "        print(\"-----------------------------------------------------\")\n",
    "        \n",
    "                        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74405604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (backbone): Sequential(\n",
       "    (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU(inplace=True)\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (pool0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu3): ReLU(inplace=True)\n",
       "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu4): ReLU(inplace=True)\n",
       "    (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu5): ReLU(inplace=True)\n",
       "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu6): ReLU(inplace=True)\n",
       "    (conv7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu7): ReLU(inplace=True)\n",
       "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VGG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65ef2b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "made_it=0\n",
    "all_layers = []\n",
    "anomalous_layers = []\n",
    "DG = tp.DependencyGraph().build_dependency(pruned_model, example_inputs=test_input)\n",
    "for group in DG.get_all_groups(ignored_layers=[], root_module_types=[nn.Conv2d]):\n",
    "    \n",
    "    dep = group[0][0]\n",
    "    layer_name = dep.target._name\n",
    "    layer= dep.target.module\n",
    "    new_layer_name = reformat_layer_name(layer_name)\n",
    " \n",
    "   \n",
    "    if(isinstance(layer,torch.nn.Conv2d)):\n",
    "       \n",
    "        \n",
    "        out_channels = layer.out_channels\n",
    "        in_channels = layer.in_channels\n",
    "        groups = layer.groups\n",
    "\n",
    "        \n",
    "        if(groups == in_channels):\n",
    "            type_conv_prune= tp.prune_depthwise_conv_in_channels\n",
    "            channels_to_prune = in_channels\n",
    "            \n",
    "        else:\n",
    "            type_conv_prune= tp.prune_conv_out_channels\n",
    "            channels_to_prune = out_channels\n",
    "            \n",
    "    \n",
    "        result_array = generate_random_array(channels_to_prune)\n",
    "        prune_layer = eval('pruned_model.'+str(new_layer_name))\n",
    "                           \n",
    "        internal_group = DG.get_pruning_group( prune_layer, type_conv_prune, idxs=result_array )\n",
    "        \n",
    "        if DG.check_pruning_group(internal_group): \n",
    "                internal_group.prune()\n",
    "        \n",
    "        try:\n",
    "            _ = pruned_model(test_input)\n",
    "            DG = tp.DependencyGraph().build_dependency(pruned_model, example_inputs=test_input)\n",
    "            made_it+=1\n",
    "            \n",
    "        except:\n",
    "            anomalous_layers.append(\"model.\"+new_layer_name)\n",
    "            print(\"Ignore:\",new_layer_name, type_conv_prune)\n",
    "      \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8ee8e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "made_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c82c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_list = list(conv_list)\n",
    "# for i in anomalous_layers:\n",
    "#     conv_list.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a21f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ipdb\n",
    "from torch_pruning.dependency import Group\n",
    "from torch_pruning.pruner.importance import Importance\n",
    "\n",
    "class FrobeniusNormImportance(Importance):\n",
    "    def __init__(self, p='fro', group_reduction='mean', normalizer='mean', bias=False):\n",
    "        self.p = p\n",
    "        self.group_reduction = group_reduction\n",
    "        self.normalizer = normalizer\n",
    "        self.bias = bias\n",
    "    def get_input_channel_importance(self, weight):\n",
    "        \n",
    "        in_channels = weight.shape[1]\n",
    "        importances = []\n",
    "        # compute the importance for each input channel\n",
    "        for i_c in range(weight.shape[1]):\n",
    "            channel_weight = weight.detach()[:, i_c]\n",
    "\n",
    "            importance = torch.norm(channel_weight)\n",
    "\n",
    "            importances.append(importance.view(1))\n",
    "        return torch.cat(importances)\n",
    "    \n",
    "    \n",
    "    def get_num_channels_to_keep(self, channels: int, prune_ratio: float) -> int:\n",
    "        \"\"\"A function to calculate the number of layers to PRESERVE after pruning\n",
    "        Note that preserve_rate = 1. - prune_ratio\n",
    "        \"\"\"\n",
    "\n",
    "        return int(round(channels * (1. - prune_ratio)))\n",
    "\n",
    "\n",
    "    def __call__(self, group: Group):\n",
    "        group_imp = []\n",
    "        group_idxs = []\n",
    "\n",
    "        for i, (dep, idxs) in enumerate(group):\n",
    "            print(group[i])\n",
    "            \n",
    "    \n",
    "#             layer = dep.layer\n",
    "#             root_idxs = group[i].root_idxs\n",
    "#             ipdb.set_trace()\n",
    "#             final_layer = group[-1].dep.target.module\n",
    "#             out_prune_dim = group[-1].dep.target.pruning_dim\n",
    "\n",
    "#             if( (isinstance(layer, (torch.nn.Conv2d))) and (isinstance(final_layer, (torch.nn.Conv2d)))):                \n",
    "\n",
    "\n",
    "                \n",
    "#                 print(layer, final_layer,out_prune_dim )\n",
    "               \n",
    "\n",
    "#                 input_conv_weights = input_conv.weight.clone().detach()\n",
    "#                 out_conv_weights = output_conv.weight.clone().detach()\n",
    "#                 out_conv_sort_imp = self.get_input_channel_importance(out_conv_weights)\n",
    "#                 sort_idx = torch.argsort(out_conv_sort_imp, descending=True) \n",
    "#                 sorted_input_conv_weights = torch.index_select(input_conv_weights, 0, sort_idx)\n",
    "\n",
    "#                 original_channels = input_conv.out_channels  # same as next_conv.in_channels\n",
    "#                 sorted_n_keep = self.get_num_channels_to_keep(original_channels, 0.30) #[:n_keep]\n",
    "#                 sorted_picks = sorted_input_conv_weights[:sorted_n_keep] #Might change Check later\n",
    "#                 local_imp = []\n",
    "#                 for sorted_pick in sorted_picks:\n",
    "#                     indx = input_conv_weights.index(sorted_pick)\n",
    "                    \n",
    "\n",
    "\n",
    "#                 ipdb.set_trace()\n",
    "# #                 group_imp.append(local_imp)\n",
    "# #                 group_idxs.append(root_idxs)\n",
    "\n",
    "\n",
    "#                 if len(group_imp) == 0:\n",
    "#                     return None\n",
    "\n",
    "#                 group_imp = torch.cat(group_imp)\n",
    "#             else:\n",
    "#                 return None\n",
    "\n",
    "#         return group_imp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79890fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/var/folders/cr/np70dxds62s1rhj2mtr8hlw80000gn/T/ipykernel_17961/3506461137.py\u001b[0m(44)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     43 \u001b[0;31m            \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 44 \u001b[0;31m            \u001b[0mfinal_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     45 \u001b[0;31m            \u001b[0mout_prune_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruning_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> exit\n"
     ]
    }
   ],
   "source": [
    "#Meta Pruning\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "import torch_pruning as tp\n",
    "\n",
    "model = VGG()\n",
    "checkpoint = torch.load('./vgg.cifar.pretrained.pth',map_location='cpu')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "example_inputs = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "\n",
    "# 1. Importance criterion\n",
    "# imp  = tp.importance.MagnitudeImportance(p=2, normalizer=None, target_types=[torch.nn.Conv2d])\n",
    "# imp  = tp.importance.GroupNormImportance(p=2,normalizer=None,group_reduction=\"first\")#normalizer=None, target_types=[torch.nn.Conv2d])\n",
    "imp  = FrobeniusNormImportance()\n",
    "# imp = tp.importance.MagnitudeImportance(p=2,normalizer=None, target_types=[torch.nn.Conv2d])\n",
    "# 2. Initialize a pruner with the model and the importance criterion\n",
    "ignored_layers = []\n",
    "for m in model.modules():\n",
    "    if isinstance(m, torch.nn.Linear) :\n",
    "        ignored_layers.append(m) # DO NOT prune the final classifier!\n",
    "        \n",
    "pruning_ratio_dict= { model.backbone.conv0 : 0.40000000000000013, model.backbone.conv1 : 0.15000000000000002, model.backbone.conv2: 0.1, model.backbone.conv3: 0.15000000000000002, model.backbone.conv4 : 0.1, model.backbone.conv5 : 0.1, model.backbone.conv6 : 0.20000000000000004} \n",
    "\n",
    "pruner = tp.pruner.MetaPruner( # We can always choose MetaPruner if sparse training is not required.\n",
    "    model,\n",
    "    example_inputs,\n",
    "    importance=imp,\n",
    "    pruning_ratio_dict = pruning_ratio_dict,\n",
    "    ignored_layers=ignored_layers,\n",
    ")\n",
    "\n",
    "pruner.step()\n",
    "model.zero_grad()\n",
    "\n",
    "# finetune the pruned model here\n",
    "# finetune(model)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9dfbfb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "62.965931863727455"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_size = 32\n",
    "transforms = {\n",
    "    \"train\": Compose([\n",
    "        RandomCrop(image_size, padding=4),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "    ]),\n",
    "    \"test\": ToTensor(),\n",
    "}\n",
    "dataset = {}\n",
    "for split in [\"train\", \"test\"]:\n",
    "  dataset[split] = CIFAR10(\n",
    "    root=\"data/cifar10\",\n",
    "    train=(split == \"train\"),\n",
    "    download=True,\n",
    "    transform=transforms[split],\n",
    "  )\n",
    "dataloader = {}\n",
    "for split in ['train', 'test']:\n",
    "  dataloader[split] = DataLoader(\n",
    "    dataset[split],\n",
    "    batch_size=512,\n",
    "    shuffle=(split == 'train'),\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "  )\n",
    "\n",
    "\n",
    "\n",
    "from sconce import sconce\n",
    "\n",
    "sconces = sconce()\n",
    "sconces.model= model # Model Definition\n",
    "sconces.criterion = nn.CrossEntropyLoss() # Loss\n",
    "sconces.optimizer= optim.Adam(sconces.model.parameters(), lr=1e-4)\n",
    "sconces.scheduler = optim.lr_scheduler.CosineAnnealingLR(sconces.optimizer, T_max=200)\n",
    "sconces.dataloader = dataloader\n",
    "sconces.epochs = 5 #Number of time we iterate over the data\n",
    "sconces.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sconces.experiment_name = \"vgg-gmp\" # Define your experiment name here\n",
    "\n",
    "sconces.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442eef9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

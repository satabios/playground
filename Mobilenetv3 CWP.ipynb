{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3249a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from collections import defaultdict, OrderedDict\n",
    "import ipdb\n",
    "import torch_pruning as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "990ff1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        counts = defaultdict(int)\n",
    "\n",
    "        def add(name: str, layer: nn.Module) -> None:\n",
    "            layers.append((f\"{name}{counts[name]}\", layer))\n",
    "            counts[name] += 1\n",
    "\n",
    "        in_channels = 3\n",
    "        for x in self.ARCH:\n",
    "            if x != 'M':\n",
    "                # conv-bn-relu\n",
    "                add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n",
    "                add(\"bn\", nn.BatchNorm2d(x))\n",
    "                add(\"relu\", nn.ReLU(True))\n",
    "                in_channels = x\n",
    "            else:\n",
    "                # maxpool\n",
    "                add(\"pool\", nn.MaxPool2d(2))\n",
    "\n",
    "        self.backbone = nn.Sequential(OrderedDict(layers))\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        # avgpool: [N, 512, 2, 2] => [N, 512]\n",
    "        x = x.mean([2, 3])\n",
    "\n",
    "        # classifier: [N, 512] => [N, 10]\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "fusing_layers = [\n",
    "    'Conv2d',\n",
    "    'BatchNorm2d',\n",
    "    'ReLU',\n",
    "    'Linear',\n",
    "    'BatchNorm1d',\n",
    "]\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def summary_string_fixed(model, all_layers, input_size, model_name=None, batch_size=-1, dtypes=None):\n",
    "    if dtypes is None:\n",
    "        dtypes = [torch.FloatTensor] * len(input_size)\n",
    "        \n",
    "    def name_fixer(names):\n",
    "\n",
    "        return_list = []\n",
    "        for string in names:\n",
    "            matches = re.finditer(r'\\.\\[(\\d+)\\]', string)\n",
    "            pop_list = [m.start(0) for m in matches]\n",
    "            pop_list.sort(reverse=True)\n",
    "            if len(pop_list) > 0:\n",
    "                string = list(string)\n",
    "                for pop_id in pop_list:\n",
    "                    string.pop(pop_id)\n",
    "                string = ''.join(string)\n",
    "            return_list.append(string)\n",
    "        return return_list\n",
    "\n",
    "    def register_hook(module, module_idx):\n",
    "        def hook(module, input, output):\n",
    "            nonlocal module_idx\n",
    "            m_key = all_layers[module_idx][0]\n",
    "            m_key = model_name + \".\" + m_key\n",
    "\n",
    "            try:\n",
    "                eval(m_key)\n",
    "            except:\n",
    "                m_key = name_fixer([m_key])[0]\n",
    "\n",
    "            summary[m_key] = OrderedDict()\n",
    "            summary[m_key][\"type\"] = str(type(module)).split('.')[-1][:-2]\n",
    "            summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "            summary[m_key][\"input_shape\"][0] = batch_size\n",
    "\n",
    "            if isinstance(output, (list, tuple)):\n",
    "                summary[m_key][\"output_shape\"] = [\n",
    "                    [-1] + list(o.size())[1:] for o in output\n",
    "                ]\n",
    "            else:\n",
    "                summary[m_key][\"output_shape\"] = list(output.size())\n",
    "                summary[m_key][\"output_shape\"][0] = batch_size\n",
    "\n",
    "            params = 0\n",
    "            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "                summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
    "                summary[m_key][\"weight_shape\"] = module.weight.shape\n",
    "            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "            summary[m_key][\"nb_params\"] = params\n",
    "\n",
    "        if (\n",
    "                not isinstance(module, nn.Sequential)\n",
    "                and not isinstance(module, nn.ModuleList)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "    if isinstance(input_size, tuple):\n",
    "        input_size = [input_size]\n",
    "\n",
    "    x = [torch.rand(2, *in_size).type(dtype)\n",
    "         for in_size, dtype in zip(input_size, dtypes)]\n",
    "\n",
    "    summary = OrderedDict()\n",
    "    hooks = []\n",
    "\n",
    "    for module_idx, (layer_name, module) in enumerate(all_layers):\n",
    "        register_hook(module, module_idx)\n",
    "\n",
    "    model(*x)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def prune_cwp(model, pruning_ratio_list):\n",
    "    \n",
    "    \n",
    "    def get_importance(layer, sparsity):\n",
    "\n",
    "        def get_input_channel_importance(weight):\n",
    "            importances = []\n",
    "            for i_c in range(weight.shape[1]):\n",
    "                channel_weight = weight.detach()[:, i_c]\n",
    "                importance = torch.norm(channel_weight)\n",
    "                importances.append(importance.view(1))\n",
    "            return torch.cat(importances)\n",
    "\n",
    "        sorted_indices = torch.argsort(get_input_channel_importance(layer.weight), descending=True)\n",
    "        n_keep = int(round(len(sorted_indices) * (1.0 - sparsity)))\n",
    "        indices_to_keep = sorted_indices[:n_keep]\n",
    "        return indices_to_keep\n",
    "    \n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    \n",
    "\n",
    "    def get_layer_name(obj):\n",
    "        if isinstance(obj, list):\n",
    "            layer_list = []\n",
    "            for internal_layer in obj:\n",
    "                layer_list.append(eval(internal_layer.replace('model', 'pruned_model')))\n",
    "            return layer_list\n",
    "        else:\n",
    "            nonlocal pruned_model\n",
    "            return eval(obj.replace('model', 'pruned_model'))\n",
    "\n",
    "    for list_ind in range(len(possible_indices_ranges)):\n",
    "        sparsity = pruning_ratio_list[list_ind]\n",
    "        layer_list = np.asarray(possible_indices_ranges[list_ind])\n",
    "\n",
    "        prev_conv = get_layer_name(layer_list[0, 0])\n",
    "        prev_bn = get_layer_name(layer_list[0, 1])\n",
    "        next_convs = [c for c in get_layer_name(list(layer_list[1:, 0]))]\n",
    "        next_bns = [b for b in get_layer_name(list(layer_list[1:-1, 1]))]  # Avoid last 0\n",
    "\n",
    "        if (len(next_bns) == 0):\n",
    "            iter_layers = zip([prev_conv, prev_bn], [next_convs, []])\n",
    "        else:\n",
    "            iter_layers = zip([prev_conv, prev_bn], [next_convs, next_bns])\n",
    "\n",
    "        importance_list_indices = get_importance(layer=next_convs[-1], sparsity=sparsity)\n",
    "\n",
    "        def prune_bn(layer, importance_list_indices):\n",
    "\n",
    "            layer.weight.set_(layer.weight.detach()[importance_list_indices])\n",
    "            layer.bias.set_(layer.bias.detach()[importance_list_indices])\n",
    "            layer.running_mean.set_(layer.running_mean.detach()[importance_list_indices])\n",
    "            layer.running_var.set_(layer.running_var.detach()[importance_list_indices])\n",
    "\n",
    "        for prev_layer, next_layers in iter_layers:\n",
    "            if(prev_layer != 0): #No BatchNorm Used:\n",
    "                \n",
    "                if (str(type(prev_layer)).split('.')[-1][:-2] == 'BatchNorm2d'):  # BatchNorm2d\n",
    "                    prune_bn(prev_layer, importance_list_indices)\n",
    "                else:\n",
    "                    prev_layer.weight.set_(prev_conv.weight.detach()[importance_list_indices, :])\n",
    "                    if prev_layer.bias is not None:\n",
    "                                bias_shape = prev_layer.weight.shape[0]\n",
    "                                prev_layer.bias = nn.Parameter(prev_layer.bias[:bias_shape])\n",
    "                    \n",
    "\n",
    "            if (len(next_layers) != 0):\n",
    "                for next_layer in next_layers:\n",
    "                    if (str(type(next_layer)).split('.')[-1][:-2] == 'BatchNorm2d'):  # BatchNorm2d\n",
    "                        prune_bn(next_layer, importance_list_indices)\n",
    "                    else:\n",
    "                        if (next_layer.weight.shape[1] == 1):\n",
    "                            \n",
    "\n",
    "                            next_layer.weight.set_(next_layer.weight.detach()[importance_list_indices, :])\n",
    "                            number_of_channels = len(importance_list_indices)\n",
    "                            next_layer.groups = number_of_channels\n",
    "                           \n",
    "                            if next_layer.bias is not None:\n",
    "                                bias_shape = next_layer.weight.shape[0]\n",
    "                                next_layer.bias = nn.Parameter(next_layer.bias[:bias_shape])\n",
    "     \n",
    "                              \n",
    "                        else:\n",
    "                            \n",
    "                            next_layer.weight.set_(next_layer.weight.detach()[:, importance_list_indices])\n",
    "#                             next_layer.groups = len(importance_list_indices)\n",
    "                            if next_layer.bias is not None:\n",
    "                                bias_shape = next_layer.weight.shape[0]\n",
    "                                next_layer.bias = nn.Parameter(next_layer.bias[:bias_shape])\n",
    "                \n",
    "#                             if(next_layer.bias!=None):\n",
    "#                                 next_layer.bias.set_(next_layer.bias[:next_layer.weight.shape[1]])\n",
    "#                                 print(len(next_layer.bias))\n",
    "\n",
    "    return pruned_model, model\n",
    "\n",
    "\n",
    "def layer_mapping(model):\n",
    "    \n",
    "    def get_all_layers(model, parent_name=''):\n",
    "        layers = []\n",
    "\n",
    "        def reformat_layer_name(str_data):\n",
    "            try:\n",
    "                split_data = str_data.split('.')\n",
    "                for ind in range(len(split_data)):\n",
    "                    data = split_data[ind]\n",
    "                    if (data.isdigit()):\n",
    "                        split_data[ind] = \"[\" + data + \"]\"\n",
    "                final_string = '.'.join(split_data)\n",
    "\n",
    "                iters_a = re.finditer(r'[a-zA-Z]\\.\\[', final_string)\n",
    "                indices = [m.start(0) + 1 for m in iters_a]\n",
    "                iters = re.finditer(r'\\]\\.\\[', final_string)\n",
    "                indices.extend([m.start(0) + 1 for m in iters])\n",
    "\n",
    "                final_string = list(final_string)\n",
    "                final_string = [final_string[i] for i in range(len(final_string)) if i not in indices]\n",
    "\n",
    "                str_data = ''.join(final_string)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            return str_data\n",
    "\n",
    "        for name, module in model.named_children():\n",
    "            full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "            test_name = \"model.\" + full_name\n",
    "            try:\n",
    "                eval(test_name)\n",
    "                layers.append((full_name, module))\n",
    "            except:\n",
    "                layers.append((reformat_layer_name(full_name), module))\n",
    "            if isinstance(module, nn.Module):\n",
    "                layers.extend(get_all_layers(module, parent_name=full_name))\n",
    "        return layers\n",
    "    all_layers = get_all_layers(model)\n",
    "    model_summary = summary_string_fixed(model, all_layers, (3, 64, 64), model_name='model')  # , device=\"cuda\")\n",
    "\n",
    "    name_type_shape = []\n",
    "    for key in model_summary.keys():\n",
    "        data = model_summary[key]\n",
    "        if (\"weight_shape\" in data.keys()):\n",
    "            name_type_shape.append([key, data['type'], data['weight_shape'][0]])\n",
    "        #     else:\n",
    "    #         name_type_shape.append([key, data['type'], 0 ])\n",
    "    name_type_shape = np.asarray(name_type_shape)\n",
    "\n",
    "    name_list = name_type_shape[:, 0]\n",
    "\n",
    "    r_name_list = np.asarray(name_list)\n",
    "    random_picks = np.random.randint(0, len(r_name_list), 10)\n",
    "    test_name_list = r_name_list[random_picks]\n",
    "    eval_hit = False\n",
    "    for layer in test_name_list:\n",
    "        try:\n",
    "            eval(layer)\n",
    "\n",
    "        except:\n",
    "            eval_hit = True\n",
    "            break\n",
    "    if (eval_hit):\n",
    "        fixed_name_list = name_fixer(r_name_list)\n",
    "        name_type_shape[:, 0] = fixed_name_list\n",
    "\n",
    "    layer_types = name_type_shape[:, 1]\n",
    "    layer_shapes = name_type_shape[:, 2]\n",
    "    mapped_layers = {'model_layer': [], 'Conv2d_BatchNorm2d_ReLU': [], 'Conv2d_BatchNorm2d': [], 'Linear_ReLU': [],\n",
    "                     'Linear_BatchNorm1d': []}\n",
    "\n",
    "    def detect_sequences(lst):\n",
    "        i = 0\n",
    "        while i < len(lst):\n",
    "\n",
    "            if i + 2 < len(lst) and [l for l in lst[i: i + 3]] == [\n",
    "                fusing_layers[0],\n",
    "                fusing_layers[1],\n",
    "                fusing_layers[2],\n",
    "            ]:\n",
    "                test_layer = layer_shapes[i: i + 2]\n",
    "                if (np.all(test_layer == test_layer[0])):\n",
    "                    mapped_layers['Conv2d_BatchNorm2d_ReLU'].append(\n",
    "                        np.take(name_list, [i for i in range(i, i + 3)]).tolist()\n",
    "                    )\n",
    "                    i += 3\n",
    "\n",
    "            elif i + 1 < len(lst) and [l for l in lst[i: i + 2]] == [\n",
    "                fusing_layers[0],\n",
    "                fusing_layers[1],\n",
    "            ]:\n",
    "                test_layer = layer_shapes[i: i + 2]\n",
    "                if (np.all(test_layer == test_layer[0])):\n",
    "                    mapped_layers['Conv2d_BatchNorm2d'].append(\n",
    "                        np.take(name_list, [i for i in range(i, i + 2)]).tolist()\n",
    "                    )\n",
    "                    i += 2\n",
    "            # if i + 1 < len(lst) and [ type(l) for l in lst[i:i+2]] == [fusing_layers[0], fusing_layers[2]]:\n",
    "            #     detected_sequences.append(np.take(name_list,[i for i in range(i,i+2)]).tolist())\n",
    "            #     i += 2\n",
    "            # elif i + 1 < len(lst) and [ type(l) for l in lst[i:i+2]] == [fusing_layers[1], fusing_layers[2]]:\n",
    "            #     detected_sequences.append(np.take(name_list,[i for i in range(i,i+2)]).tolist())\n",
    "            #     i += 2\n",
    "            elif i + 1 < len(lst) and [l for l in lst[i: i + 2]] == [\n",
    "                fusing_layers[3],\n",
    "                fusing_layers[2],\n",
    "            ]:\n",
    "                mapped_layers['Linear_ReLU'].append(\n",
    "                    np.take(name_list, [i for i in range(i, i + 2)]).tolist()\n",
    "                )\n",
    "                i += 2\n",
    "            elif i + 1 < len(lst) and [l for l in lst[i: i + 2]] == [\n",
    "                fusing_layers[3],\n",
    "                fusing_layers[4],\n",
    "            ]:\n",
    "                mapped_layers['Linear_BatchNorm1d'].append(\n",
    "                    np.take(name_list, [i for i in range(i, i + 2)]).tolist()\n",
    "                )\n",
    "                i += 2\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    detect_sequences(layer_types)\n",
    "\n",
    "    for keys, value in mapped_layers.items():\n",
    "        mapped_layers[keys] = np.asarray(mapped_layers[keys])\n",
    "\n",
    "    mapped_layers['name_type_shape'] = name_type_shape\n",
    "    # self.mapped_layers = mapped_layers\n",
    "\n",
    "    # CWP\n",
    "    keys_to_lookout = ['Conv2d_BatchNorm2d_ReLU', 'Conv2d_BatchNorm2d']\n",
    "    pruning_layer_of_interest, qat_layer_of_interest = [], []\n",
    "\n",
    "    # CWP or QAT Fusion Layers\n",
    "    for keys in keys_to_lookout:\n",
    "        data = mapped_layers[keys]\n",
    "        if (len(data) != 0):\n",
    "            qat_layer_of_interest.append(data)\n",
    "    mapped_layers['qat_layers'] = np.asarray(qat_layer_of_interest)\n",
    "\n",
    "    return mapped_layers\n",
    "\n",
    "\n",
    "\n",
    "# GMP\n",
    "#         layer_of_interest=mapped_layers['name_type_shape'][:,0] # all layers with weights\n",
    "#         Check for all with weights\n",
    "# Wanda\n",
    "\n",
    "# def string_fixer(name_list):\n",
    "#     for ind in range(len(name_list)):\n",
    "#         modified_string = re.sub(r'\\.(\\[)', r'\\1', name_list[ind])\n",
    "#         name_list[ind] = modified_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c775e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def cwp_possible_layers(layer_name_list):\n",
    "    possible_indices = []\n",
    "    idx = 0\n",
    "    \n",
    "    while idx < len(layer_name_list):\n",
    "        current_value = layer_name_list[idx]\n",
    "        layer_shape = eval(current_value).weight.shape\n",
    "        curr_merge_list = []\n",
    "        curr_merge_list.append([current_value, 0])\n",
    "        hit_catch = False\n",
    "        for internal_idx in range(idx + 1, len(layer_name_list) - 1):\n",
    "            new_layer = layer_name_list[internal_idx]\n",
    "            new_layer_shape = eval(new_layer).weight.shape\n",
    "            if len(new_layer_shape) == 4:\n",
    "                curr_merge_list.append([new_layer, 0])\n",
    "                if layer_shape[0] == new_layer_shape[1]:\n",
    "                    hit_catch = True\n",
    "                    break\n",
    "            elif len(new_layer_shape) == 1:\n",
    "                curr_merge_list[len(curr_merge_list) - 1][1] = new_layer\n",
    "        possible_indices.append(curr_merge_list)\n",
    "        if hit_catch == True:\n",
    "            idx = internal_idx\n",
    "        else:\n",
    "            idx += 1\n",
    "    return possible_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5de38ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/sathya/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sathya/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# #load the pretrained model\n",
    "\n",
    "# resnet18 = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n",
    "# densenet = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=False)\n",
    "\n",
    "# super_net_name = \"ofa_supernet_mbv3_w10\" \n",
    "\n",
    "vgg = VGG()\n",
    "mobilenet_v2 = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "mobilenet_v3 = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v3_small', pretrained=True)\n",
    "# # super_net = torch.hub.load('mit-han-lab/once-for-all', super_net_name, pretrained=True)\n",
    "# model_list = [ mobilenet_v3 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "301fc1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_ratio = 0.95\n",
    "\n",
    "model = copy.deepcopy(mobilenet_v3)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "mapped_layers = layer_mapping(model)\n",
    "name_list = mapped_layers['name_type_shape'][:, 0]\n",
    "\n",
    "possible_indices_ranges = cwp_possible_layers(name_list)\n",
    "possible_indices_ranges = [lst for lst in possible_indices_ranges if len(lst) > 1]\n",
    "    \n",
    "pruning_layer_length = len(possible_indices_ranges)\n",
    "pruning_ratio_list = (pruning_layer_length) * [pruning_ratio]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "82629e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['model.features[0][0]', 'model.features[0][1]',\n",
       "       'model.features[1].block[0][0]', 'model.features[1].block[0][1]',\n",
       "       'model.features[1].block[1].fc1', 'model.features[1].block[1].fc2',\n",
       "       'model.features[1].block[2][0]', 'model.features[1].block[2][1]',\n",
       "       'model.features[2].block[0][0]', 'model.features[2].block[0][1]',\n",
       "       'model.features[2].block[1][0]', 'model.features[2].block[1][1]',\n",
       "       'model.features[2].block[2][0]', 'model.features[2].block[2][1]',\n",
       "       'model.features[3].block[0][0]', 'model.features[3].block[0][1]',\n",
       "       'model.features[3].block[1][0]', 'model.features[3].block[1][1]',\n",
       "       'model.features[3].block[2][0]', 'model.features[3].block[2][1]',\n",
       "       'model.features[4].block[0][0]', 'model.features[4].block[0][1]',\n",
       "       'model.features[4].block[1][0]', 'model.features[4].block[1][1]',\n",
       "       'model.features[4].block[2].fc1', 'model.features[4].block[2].fc2',\n",
       "       'model.features[4].block[3][0]', 'model.features[4].block[3][1]',\n",
       "       'model.features[5].block[0][0]', 'model.features[5].block[0][1]',\n",
       "       'model.features[5].block[1][0]', 'model.features[5].block[1][1]',\n",
       "       'model.features[5].block[2].fc1', 'model.features[5].block[2].fc2',\n",
       "       'model.features[5].block[3][0]', 'model.features[5].block[3][1]',\n",
       "       'model.features[6].block[0][0]', 'model.features[6].block[0][1]',\n",
       "       'model.features[6].block[1][0]', 'model.features[6].block[1][1]',\n",
       "       'model.features[6].block[2].fc1', 'model.features[6].block[2].fc2',\n",
       "       'model.features[6].block[3][0]', 'model.features[6].block[3][1]',\n",
       "       'model.features[7].block[0][0]', 'model.features[7].block[0][1]',\n",
       "       'model.features[7].block[1][0]', 'model.features[7].block[1][1]',\n",
       "       'model.features[7].block[2].fc1', 'model.features[7].block[2].fc2',\n",
       "       'model.features[7].block[3][0]', 'model.features[7].block[3][1]',\n",
       "       'model.features[8].block[0][0]', 'model.features[8].block[0][1]',\n",
       "       'model.features[8].block[1][0]', 'model.features[8].block[1][1]',\n",
       "       'model.features[8].block[2].fc1', 'model.features[8].block[2].fc2',\n",
       "       'model.features[8].block[3][0]', 'model.features[8].block[3][1]',\n",
       "       'model.features[9].block[0][0]', 'model.features[9].block[0][1]',\n",
       "       'model.features[9].block[1][0]', 'model.features[9].block[1][1]',\n",
       "       'model.features[9].block[2].fc1', 'model.features[9].block[2].fc2',\n",
       "       'model.features[9].block[3][0]', 'model.features[9].block[3][1]',\n",
       "       'model.features[10].block[0][0]', 'model.features[10].block[0][1]',\n",
       "       'model.features[10].block[1][0]', 'model.features[10].block[1][1]',\n",
       "       'model.features[10].block[2].fc1',\n",
       "       'model.features[10].block[2].fc2',\n",
       "       'model.features[10].block[3][0]', 'model.features[10].block[3][1]',\n",
       "       'model.features[11].block[0][0]', 'model.features[11].block[0][1]',\n",
       "       'model.features[11].block[1][0]', 'model.features[11].block[1][1]',\n",
       "       'model.features[11].block[2].fc1',\n",
       "       'model.features[11].block[2].fc2',\n",
       "       'model.features[11].block[3][0]', 'model.features[11].block[3][1]',\n",
       "       'model.features[12][0]', 'model.features[12][1]',\n",
       "       'model.classifier[0]', 'model.classifier[3]'], dtype='<U31')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f96366ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = []\n",
    "for ind in range(len(name_list)):\n",
    "    layer_name= name_list[ind]\n",
    "    layer= eval(layer_name)\n",
    "    if(isinstance(layer, torch.nn.modules.conv.Conv2d)):\n",
    "        conv_layers.append(layer_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cc96e457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.features[0][0]',\n",
       " 'model.features[1].block[0][0]',\n",
       " 'model.features[1].block[1].fc1',\n",
       " 'model.features[1].block[1].fc2',\n",
       " 'model.features[1].block[2][0]',\n",
       " 'model.features[2].block[0][0]',\n",
       " 'model.features[2].block[1][0]',\n",
       " 'model.features[2].block[2][0]',\n",
       " 'model.features[3].block[0][0]',\n",
       " 'model.features[3].block[1][0]',\n",
       " 'model.features[3].block[2][0]',\n",
       " 'model.features[4].block[0][0]',\n",
       " 'model.features[4].block[1][0]',\n",
       " 'model.features[4].block[2].fc1',\n",
       " 'model.features[4].block[2].fc2',\n",
       " 'model.features[4].block[3][0]',\n",
       " 'model.features[5].block[0][0]',\n",
       " 'model.features[5].block[1][0]',\n",
       " 'model.features[5].block[2].fc1',\n",
       " 'model.features[5].block[2].fc2',\n",
       " 'model.features[5].block[3][0]',\n",
       " 'model.features[6].block[0][0]',\n",
       " 'model.features[6].block[1][0]',\n",
       " 'model.features[6].block[2].fc1',\n",
       " 'model.features[6].block[2].fc2',\n",
       " 'model.features[6].block[3][0]',\n",
       " 'model.features[7].block[0][0]',\n",
       " 'model.features[7].block[1][0]',\n",
       " 'model.features[7].block[2].fc1',\n",
       " 'model.features[7].block[2].fc2',\n",
       " 'model.features[7].block[3][0]',\n",
       " 'model.features[8].block[0][0]',\n",
       " 'model.features[8].block[1][0]',\n",
       " 'model.features[8].block[2].fc1',\n",
       " 'model.features[8].block[2].fc2',\n",
       " 'model.features[8].block[3][0]',\n",
       " 'model.features[9].block[0][0]',\n",
       " 'model.features[9].block[1][0]',\n",
       " 'model.features[9].block[2].fc1',\n",
       " 'model.features[9].block[2].fc2',\n",
       " 'model.features[9].block[3][0]',\n",
       " 'model.features[10].block[0][0]',\n",
       " 'model.features[10].block[1][0]',\n",
       " 'model.features[10].block[2].fc1',\n",
       " 'model.features[10].block[2].fc2',\n",
       " 'model.features[10].block[3][0]',\n",
       " 'model.features[11].block[0][0]',\n",
       " 'model.features[11].block[1][0]',\n",
       " 'model.features[11].block[2].fc1',\n",
       " 'model.features[11].block[2].fc2',\n",
       " 'model.features[11].block[3][0]',\n",
       " 'model.features[12][0]']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d0beebb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 16, 16]             432\n",
      "       BatchNorm2d-2           [-1, 16, 16, 16]              32\n",
      "         Hardswish-3           [-1, 16, 16, 16]               0\n",
      "            Conv2d-4             [-1, 16, 8, 8]             144\n",
      "       BatchNorm2d-5             [-1, 16, 8, 8]              32\n",
      "              ReLU-6             [-1, 16, 8, 8]               0\n",
      " AdaptiveAvgPool2d-7             [-1, 16, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]             136\n",
      "              ReLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10             [-1, 16, 1, 1]             144\n",
      "      Hardsigmoid-11             [-1, 16, 1, 1]               0\n",
      "SqueezeExcitation-12             [-1, 16, 8, 8]               0\n",
      "           Conv2d-13             [-1, 16, 8, 8]             256\n",
      "      BatchNorm2d-14             [-1, 16, 8, 8]              32\n",
      " InvertedResidual-15             [-1, 16, 8, 8]               0\n",
      "           Conv2d-16             [-1, 72, 8, 8]           1,152\n",
      "      BatchNorm2d-17             [-1, 72, 8, 8]             144\n",
      "             ReLU-18             [-1, 72, 8, 8]               0\n",
      "           Conv2d-19             [-1, 72, 4, 4]             648\n",
      "      BatchNorm2d-20             [-1, 72, 4, 4]             144\n",
      "             ReLU-21             [-1, 72, 4, 4]               0\n",
      "           Conv2d-22             [-1, 24, 4, 4]           1,728\n",
      "      BatchNorm2d-23             [-1, 24, 4, 4]              48\n",
      " InvertedResidual-24             [-1, 24, 4, 4]               0\n",
      "           Conv2d-25             [-1, 88, 4, 4]           2,112\n",
      "      BatchNorm2d-26             [-1, 88, 4, 4]             176\n",
      "             ReLU-27             [-1, 88, 4, 4]               0\n",
      "           Conv2d-28             [-1, 88, 4, 4]             792\n",
      "      BatchNorm2d-29             [-1, 88, 4, 4]             176\n",
      "             ReLU-30             [-1, 88, 4, 4]               0\n",
      "           Conv2d-31             [-1, 24, 4, 4]           2,112\n",
      "      BatchNorm2d-32             [-1, 24, 4, 4]              48\n",
      " InvertedResidual-33             [-1, 24, 4, 4]               0\n",
      "           Conv2d-34             [-1, 96, 4, 4]           2,304\n",
      "      BatchNorm2d-35             [-1, 96, 4, 4]             192\n",
      "        Hardswish-36             [-1, 96, 4, 4]               0\n",
      "           Conv2d-37             [-1, 96, 2, 2]           2,400\n",
      "      BatchNorm2d-38             [-1, 96, 2, 2]             192\n",
      "        Hardswish-39             [-1, 96, 2, 2]               0\n",
      "AdaptiveAvgPool2d-40             [-1, 96, 1, 1]               0\n",
      "           Conv2d-41             [-1, 24, 1, 1]           2,328\n",
      "             ReLU-42             [-1, 24, 1, 1]               0\n",
      "           Conv2d-43             [-1, 96, 1, 1]           2,400\n",
      "      Hardsigmoid-44             [-1, 96, 1, 1]               0\n",
      "SqueezeExcitation-45             [-1, 96, 2, 2]               0\n",
      "           Conv2d-46             [-1, 40, 2, 2]           3,840\n",
      "      BatchNorm2d-47             [-1, 40, 2, 2]              80\n",
      " InvertedResidual-48             [-1, 40, 2, 2]               0\n",
      "           Conv2d-49            [-1, 240, 2, 2]           9,600\n",
      "      BatchNorm2d-50            [-1, 240, 2, 2]             480\n",
      "        Hardswish-51            [-1, 240, 2, 2]               0\n",
      "           Conv2d-52            [-1, 240, 2, 2]           6,000\n",
      "      BatchNorm2d-53            [-1, 240, 2, 2]             480\n",
      "        Hardswish-54            [-1, 240, 2, 2]               0\n",
      "AdaptiveAvgPool2d-55            [-1, 240, 1, 1]               0\n",
      "           Conv2d-56             [-1, 64, 1, 1]          15,424\n",
      "             ReLU-57             [-1, 64, 1, 1]               0\n",
      "           Conv2d-58            [-1, 240, 1, 1]          15,600\n",
      "      Hardsigmoid-59            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-60            [-1, 240, 2, 2]               0\n",
      "           Conv2d-61             [-1, 40, 2, 2]           9,600\n",
      "      BatchNorm2d-62             [-1, 40, 2, 2]              80\n",
      " InvertedResidual-63             [-1, 40, 2, 2]               0\n",
      "           Conv2d-64            [-1, 240, 2, 2]           9,600\n",
      "      BatchNorm2d-65            [-1, 240, 2, 2]             480\n",
      "        Hardswish-66            [-1, 240, 2, 2]               0\n",
      "           Conv2d-67            [-1, 240, 2, 2]           6,000\n",
      "      BatchNorm2d-68            [-1, 240, 2, 2]             480\n",
      "        Hardswish-69            [-1, 240, 2, 2]               0\n",
      "AdaptiveAvgPool2d-70            [-1, 240, 1, 1]               0\n",
      "           Conv2d-71             [-1, 64, 1, 1]          15,424\n",
      "             ReLU-72             [-1, 64, 1, 1]               0\n",
      "           Conv2d-73            [-1, 240, 1, 1]          15,600\n",
      "      Hardsigmoid-74            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-75            [-1, 240, 2, 2]               0\n",
      "           Conv2d-76             [-1, 40, 2, 2]           9,600\n",
      "      BatchNorm2d-77             [-1, 40, 2, 2]              80\n",
      " InvertedResidual-78             [-1, 40, 2, 2]               0\n",
      "           Conv2d-79            [-1, 120, 2, 2]           4,800\n",
      "      BatchNorm2d-80            [-1, 120, 2, 2]             240\n",
      "        Hardswish-81            [-1, 120, 2, 2]               0\n",
      "           Conv2d-82            [-1, 120, 2, 2]           3,000\n",
      "      BatchNorm2d-83            [-1, 120, 2, 2]             240\n",
      "        Hardswish-84            [-1, 120, 2, 2]               0\n",
      "AdaptiveAvgPool2d-85            [-1, 120, 1, 1]               0\n",
      "           Conv2d-86             [-1, 32, 1, 1]           3,872\n",
      "             ReLU-87             [-1, 32, 1, 1]               0\n",
      "           Conv2d-88            [-1, 120, 1, 1]           3,960\n",
      "      Hardsigmoid-89            [-1, 120, 1, 1]               0\n",
      "SqueezeExcitation-90            [-1, 120, 2, 2]               0\n",
      "           Conv2d-91             [-1, 48, 2, 2]           5,760\n",
      "      BatchNorm2d-92             [-1, 48, 2, 2]              96\n",
      " InvertedResidual-93             [-1, 48, 2, 2]               0\n",
      "           Conv2d-94            [-1, 144, 2, 2]           6,912\n",
      "      BatchNorm2d-95            [-1, 144, 2, 2]             288\n",
      "        Hardswish-96            [-1, 144, 2, 2]               0\n",
      "           Conv2d-97            [-1, 144, 2, 2]           3,600\n",
      "      BatchNorm2d-98            [-1, 144, 2, 2]             288\n",
      "        Hardswish-99            [-1, 144, 2, 2]               0\n",
      "AdaptiveAvgPool2d-100            [-1, 144, 1, 1]               0\n",
      "          Conv2d-101             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-102             [-1, 40, 1, 1]               0\n",
      "          Conv2d-103            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-104            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-105            [-1, 144, 2, 2]               0\n",
      "          Conv2d-106             [-1, 48, 2, 2]           6,912\n",
      "     BatchNorm2d-107             [-1, 48, 2, 2]              96\n",
      "InvertedResidual-108             [-1, 48, 2, 2]               0\n",
      "          Conv2d-109            [-1, 288, 2, 2]          13,824\n",
      "     BatchNorm2d-110            [-1, 288, 2, 2]             576\n",
      "       Hardswish-111            [-1, 288, 2, 2]               0\n",
      "          Conv2d-112            [-1, 288, 1, 1]           7,200\n",
      "     BatchNorm2d-113            [-1, 288, 1, 1]             576\n",
      "       Hardswish-114            [-1, 288, 1, 1]               0\n",
      "AdaptiveAvgPool2d-115            [-1, 288, 1, 1]               0\n",
      "          Conv2d-116             [-1, 72, 1, 1]          20,808\n",
      "            ReLU-117             [-1, 72, 1, 1]               0\n",
      "          Conv2d-118            [-1, 288, 1, 1]          21,024\n",
      "     Hardsigmoid-119            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-120            [-1, 288, 1, 1]               0\n",
      "          Conv2d-121             [-1, 96, 1, 1]          27,648\n",
      "     BatchNorm2d-122             [-1, 96, 1, 1]             192\n",
      "InvertedResidual-123             [-1, 96, 1, 1]               0\n",
      "          Conv2d-124            [-1, 576, 1, 1]          55,296\n",
      "     BatchNorm2d-125            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-126            [-1, 576, 1, 1]               0\n",
      "          Conv2d-127            [-1, 576, 1, 1]          14,400\n",
      "     BatchNorm2d-128            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-129            [-1, 576, 1, 1]               0\n",
      "AdaptiveAvgPool2d-130            [-1, 576, 1, 1]               0\n",
      "          Conv2d-131            [-1, 144, 1, 1]          83,088\n",
      "            ReLU-132            [-1, 144, 1, 1]               0\n",
      "          Conv2d-133            [-1, 576, 1, 1]          83,520\n",
      "     Hardsigmoid-134            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-135            [-1, 576, 1, 1]               0\n",
      "          Conv2d-136             [-1, 96, 1, 1]          55,296\n",
      "     BatchNorm2d-137             [-1, 96, 1, 1]             192\n",
      "InvertedResidual-138             [-1, 96, 1, 1]               0\n",
      "          Conv2d-139            [-1, 576, 1, 1]          55,296\n",
      "     BatchNorm2d-140            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-141            [-1, 576, 1, 1]               0\n",
      "          Conv2d-142            [-1, 576, 1, 1]          14,400\n",
      "     BatchNorm2d-143            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-144            [-1, 576, 1, 1]               0\n",
      "AdaptiveAvgPool2d-145            [-1, 576, 1, 1]               0\n",
      "          Conv2d-146            [-1, 144, 1, 1]          83,088\n",
      "            ReLU-147            [-1, 144, 1, 1]               0\n",
      "          Conv2d-148            [-1, 576, 1, 1]          83,520\n",
      "     Hardsigmoid-149            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-150            [-1, 576, 1, 1]               0\n",
      "          Conv2d-151             [-1, 96, 1, 1]          55,296\n",
      "     BatchNorm2d-152             [-1, 96, 1, 1]             192\n",
      "InvertedResidual-153             [-1, 96, 1, 1]               0\n",
      "          Conv2d-154            [-1, 576, 1, 1]          55,296\n",
      "     BatchNorm2d-155            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-156            [-1, 576, 1, 1]               0\n",
      "AdaptiveAvgPool2d-157            [-1, 576, 1, 1]               0\n",
      "          Linear-158                 [-1, 1024]         590,848\n",
      "       Hardswish-159                 [-1, 1024]               0\n",
      "         Dropout-160                 [-1, 1024]               0\n",
      "          Linear-161                 [-1, 1000]       1,025,000\n",
      "================================================================\n",
      "Total params: 2,542,856\n",
      "Trainable params: 2,542,856\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.80\n",
      "Params size (MB): 9.70\n",
      "Estimated Total Size (MB): 10.51\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fa21bb19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([144, 576, 1, 1]),\n",
       " torch.Size([576, 144, 1, 1]),\n",
       " torch.Size([96, 576, 1, 1]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval('model.features[11].block[2].fc1').weight.shape, eval('model.features[11].block[2].fc2').weight.shape, eval('model.features[11].block[3][0]').weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7b9c0457",
   "metadata": {},
   "outputs": [],
   "source": [
    "DG = tp.DependencyGraph().build_dependency(model, example_inputs=example_inputs)\n",
    "\n",
    "# 2. Select some channels to prune. Here we prune the channels indexed by [2, 6, 9].\n",
    "pruning_idxs =[2, 6, 9,0,13,16,15,20,23,7,32]\n",
    "\n",
    "pruning_group = DG.get_pruning_group( eval('model.features[11].block[2].fc2'), tp.prune_conv_out_channels, idxs=pruning_idxs )\n",
    "\n",
    "# 3. prune all grouped layer that is coupled with model.conv1\n",
    "if DG.check_pruning_group(pruning_group):\n",
    "    pruning_group.prune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "965e6edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "436aef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pruned_model.state_dict(),'pruned.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bf0e14df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([144, 565, 1, 1]),\n",
       " torch.Size([565, 144, 1, 1]),\n",
       " torch.Size([96, 565, 1, 1]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval('pruned_model.features[11].block[2].fc1').weight.shape, eval('pruned_model.features[11].block[2].fc2').weight.shape, eval('pruned_model.features[11].block[3][0]').weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f6e5a5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 16, 16]             432\n",
      "       BatchNorm2d-2           [-1, 16, 16, 16]              32\n",
      "         Hardswish-3           [-1, 16, 16, 16]               0\n",
      "            Conv2d-4             [-1, 16, 8, 8]             144\n",
      "       BatchNorm2d-5             [-1, 16, 8, 8]              32\n",
      "              ReLU-6             [-1, 16, 8, 8]               0\n",
      " AdaptiveAvgPool2d-7             [-1, 16, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]             136\n",
      "              ReLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10             [-1, 16, 1, 1]             144\n",
      "      Hardsigmoid-11             [-1, 16, 1, 1]               0\n",
      "SqueezeExcitation-12             [-1, 16, 8, 8]               0\n",
      "           Conv2d-13             [-1, 16, 8, 8]             256\n",
      "      BatchNorm2d-14             [-1, 16, 8, 8]              32\n",
      " InvertedResidual-15             [-1, 16, 8, 8]               0\n",
      "           Conv2d-16             [-1, 72, 8, 8]           1,152\n",
      "      BatchNorm2d-17             [-1, 72, 8, 8]             144\n",
      "             ReLU-18             [-1, 72, 8, 8]               0\n",
      "           Conv2d-19             [-1, 72, 4, 4]             648\n",
      "      BatchNorm2d-20             [-1, 72, 4, 4]             144\n",
      "             ReLU-21             [-1, 72, 4, 4]               0\n",
      "           Conv2d-22             [-1, 24, 4, 4]           1,728\n",
      "      BatchNorm2d-23             [-1, 24, 4, 4]              48\n",
      " InvertedResidual-24             [-1, 24, 4, 4]               0\n",
      "           Conv2d-25             [-1, 88, 4, 4]           2,112\n",
      "      BatchNorm2d-26             [-1, 88, 4, 4]             176\n",
      "             ReLU-27             [-1, 88, 4, 4]               0\n",
      "           Conv2d-28             [-1, 88, 4, 4]             792\n",
      "      BatchNorm2d-29             [-1, 88, 4, 4]             176\n",
      "             ReLU-30             [-1, 88, 4, 4]               0\n",
      "           Conv2d-31             [-1, 24, 4, 4]           2,112\n",
      "      BatchNorm2d-32             [-1, 24, 4, 4]              48\n",
      " InvertedResidual-33             [-1, 24, 4, 4]               0\n",
      "           Conv2d-34             [-1, 96, 4, 4]           2,304\n",
      "      BatchNorm2d-35             [-1, 96, 4, 4]             192\n",
      "        Hardswish-36             [-1, 96, 4, 4]               0\n",
      "           Conv2d-37             [-1, 96, 2, 2]           2,400\n",
      "      BatchNorm2d-38             [-1, 96, 2, 2]             192\n",
      "        Hardswish-39             [-1, 96, 2, 2]               0\n",
      "AdaptiveAvgPool2d-40             [-1, 96, 1, 1]               0\n",
      "           Conv2d-41             [-1, 24, 1, 1]           2,328\n",
      "             ReLU-42             [-1, 24, 1, 1]               0\n",
      "           Conv2d-43             [-1, 96, 1, 1]           2,400\n",
      "      Hardsigmoid-44             [-1, 96, 1, 1]               0\n",
      "SqueezeExcitation-45             [-1, 96, 2, 2]               0\n",
      "           Conv2d-46             [-1, 40, 2, 2]           3,840\n",
      "      BatchNorm2d-47             [-1, 40, 2, 2]              80\n",
      " InvertedResidual-48             [-1, 40, 2, 2]               0\n",
      "           Conv2d-49            [-1, 240, 2, 2]           9,600\n",
      "      BatchNorm2d-50            [-1, 240, 2, 2]             480\n",
      "        Hardswish-51            [-1, 240, 2, 2]               0\n",
      "           Conv2d-52            [-1, 240, 2, 2]           6,000\n",
      "      BatchNorm2d-53            [-1, 240, 2, 2]             480\n",
      "        Hardswish-54            [-1, 240, 2, 2]               0\n",
      "AdaptiveAvgPool2d-55            [-1, 240, 1, 1]               0\n",
      "           Conv2d-56             [-1, 64, 1, 1]          15,424\n",
      "             ReLU-57             [-1, 64, 1, 1]               0\n",
      "           Conv2d-58            [-1, 240, 1, 1]          15,600\n",
      "      Hardsigmoid-59            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-60            [-1, 240, 2, 2]               0\n",
      "           Conv2d-61             [-1, 40, 2, 2]           9,600\n",
      "      BatchNorm2d-62             [-1, 40, 2, 2]              80\n",
      " InvertedResidual-63             [-1, 40, 2, 2]               0\n",
      "           Conv2d-64            [-1, 240, 2, 2]           9,600\n",
      "      BatchNorm2d-65            [-1, 240, 2, 2]             480\n",
      "        Hardswish-66            [-1, 240, 2, 2]               0\n",
      "           Conv2d-67            [-1, 240, 2, 2]           6,000\n",
      "      BatchNorm2d-68            [-1, 240, 2, 2]             480\n",
      "        Hardswish-69            [-1, 240, 2, 2]               0\n",
      "AdaptiveAvgPool2d-70            [-1, 240, 1, 1]               0\n",
      "           Conv2d-71             [-1, 64, 1, 1]          15,424\n",
      "             ReLU-72             [-1, 64, 1, 1]               0\n",
      "           Conv2d-73            [-1, 240, 1, 1]          15,600\n",
      "      Hardsigmoid-74            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-75            [-1, 240, 2, 2]               0\n",
      "           Conv2d-76             [-1, 40, 2, 2]           9,600\n",
      "      BatchNorm2d-77             [-1, 40, 2, 2]              80\n",
      " InvertedResidual-78             [-1, 40, 2, 2]               0\n",
      "           Conv2d-79            [-1, 120, 2, 2]           4,800\n",
      "      BatchNorm2d-80            [-1, 120, 2, 2]             240\n",
      "        Hardswish-81            [-1, 120, 2, 2]               0\n",
      "           Conv2d-82            [-1, 120, 2, 2]           3,000\n",
      "      BatchNorm2d-83            [-1, 120, 2, 2]             240\n",
      "        Hardswish-84            [-1, 120, 2, 2]               0\n",
      "AdaptiveAvgPool2d-85            [-1, 120, 1, 1]               0\n",
      "           Conv2d-86             [-1, 32, 1, 1]           3,872\n",
      "             ReLU-87             [-1, 32, 1, 1]               0\n",
      "           Conv2d-88            [-1, 120, 1, 1]           3,960\n",
      "      Hardsigmoid-89            [-1, 120, 1, 1]               0\n",
      "SqueezeExcitation-90            [-1, 120, 2, 2]               0\n",
      "           Conv2d-91             [-1, 48, 2, 2]           5,760\n",
      "      BatchNorm2d-92             [-1, 48, 2, 2]              96\n",
      " InvertedResidual-93             [-1, 48, 2, 2]               0\n",
      "           Conv2d-94            [-1, 144, 2, 2]           6,912\n",
      "      BatchNorm2d-95            [-1, 144, 2, 2]             288\n",
      "        Hardswish-96            [-1, 144, 2, 2]               0\n",
      "           Conv2d-97            [-1, 144, 2, 2]           3,600\n",
      "      BatchNorm2d-98            [-1, 144, 2, 2]             288\n",
      "        Hardswish-99            [-1, 144, 2, 2]               0\n",
      "AdaptiveAvgPool2d-100            [-1, 144, 1, 1]               0\n",
      "          Conv2d-101             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-102             [-1, 40, 1, 1]               0\n",
      "          Conv2d-103            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-104            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-105            [-1, 144, 2, 2]               0\n",
      "          Conv2d-106             [-1, 48, 2, 2]           6,912\n",
      "     BatchNorm2d-107             [-1, 48, 2, 2]              96\n",
      "InvertedResidual-108             [-1, 48, 2, 2]               0\n",
      "          Conv2d-109            [-1, 288, 2, 2]          13,824\n",
      "     BatchNorm2d-110            [-1, 288, 2, 2]             576\n",
      "       Hardswish-111            [-1, 288, 2, 2]               0\n",
      "          Conv2d-112            [-1, 288, 1, 1]           7,200\n",
      "     BatchNorm2d-113            [-1, 288, 1, 1]             576\n",
      "       Hardswish-114            [-1, 288, 1, 1]               0\n",
      "AdaptiveAvgPool2d-115            [-1, 288, 1, 1]               0\n",
      "          Conv2d-116             [-1, 72, 1, 1]          20,808\n",
      "            ReLU-117             [-1, 72, 1, 1]               0\n",
      "          Conv2d-118            [-1, 288, 1, 1]          21,024\n",
      "     Hardsigmoid-119            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-120            [-1, 288, 1, 1]               0\n",
      "          Conv2d-121             [-1, 96, 1, 1]          27,648\n",
      "     BatchNorm2d-122             [-1, 96, 1, 1]             192\n",
      "InvertedResidual-123             [-1, 96, 1, 1]               0\n",
      "          Conv2d-124            [-1, 576, 1, 1]          55,296\n",
      "     BatchNorm2d-125            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-126            [-1, 576, 1, 1]               0\n",
      "          Conv2d-127            [-1, 576, 1, 1]          14,400\n",
      "     BatchNorm2d-128            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-129            [-1, 576, 1, 1]               0\n",
      "AdaptiveAvgPool2d-130            [-1, 576, 1, 1]               0\n",
      "          Conv2d-131            [-1, 144, 1, 1]          83,088\n",
      "            ReLU-132            [-1, 144, 1, 1]               0\n",
      "          Conv2d-133            [-1, 576, 1, 1]          83,520\n",
      "     Hardsigmoid-134            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-135            [-1, 576, 1, 1]               0\n",
      "          Conv2d-136             [-1, 96, 1, 1]          55,296\n",
      "     BatchNorm2d-137             [-1, 96, 1, 1]             192\n",
      "InvertedResidual-138             [-1, 96, 1, 1]               0\n",
      "          Conv2d-139            [-1, 565, 1, 1]          54,240\n",
      "     BatchNorm2d-140            [-1, 565, 1, 1]           1,130\n",
      "       Hardswish-141            [-1, 565, 1, 1]               0\n",
      "          Conv2d-142            [-1, 565, 1, 1]          14,125\n",
      "     BatchNorm2d-143            [-1, 565, 1, 1]           1,130\n",
      "       Hardswish-144            [-1, 565, 1, 1]               0\n",
      "AdaptiveAvgPool2d-145            [-1, 565, 1, 1]               0\n",
      "          Conv2d-146            [-1, 144, 1, 1]          81,504\n",
      "            ReLU-147            [-1, 144, 1, 1]               0\n",
      "          Conv2d-148            [-1, 565, 1, 1]          81,925\n",
      "     Hardsigmoid-149            [-1, 565, 1, 1]               0\n",
      "SqueezeExcitation-150            [-1, 565, 1, 1]               0\n",
      "          Conv2d-151             [-1, 96, 1, 1]          54,240\n",
      "     BatchNorm2d-152             [-1, 96, 1, 1]             192\n",
      "InvertedResidual-153             [-1, 96, 1, 1]               0\n",
      "          Conv2d-154            [-1, 576, 1, 1]          55,296\n",
      "     BatchNorm2d-155            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-156            [-1, 576, 1, 1]               0\n",
      "AdaptiveAvgPool2d-157            [-1, 576, 1, 1]               0\n",
      "          Linear-158                 [-1, 1024]         590,848\n",
      "       Hardswish-159                 [-1, 1024]               0\n",
      "         Dropout-160                 [-1, 1024]               0\n",
      "          Linear-161                 [-1, 1000]       1,025,000\n",
      "================================================================\n",
      "Total params: 2,537,246\n",
      "Trainable params: 2,537,246\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.80\n",
      "Params size (MB): 9.68\n",
      "Estimated Total Size (MB): 10.49\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(pruned_model,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "667615a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "L: 0\n",
      "1\n",
      "L: 1\n",
      "2\n",
      "L: 2\n",
      "3\n",
      "L: 3\n",
      "4\n",
      "L: 4\n",
      "5\n",
      "L: 5\n",
      "6\n",
      "L: 6\n",
      "7\n",
      "L: 7\n",
      "8\n",
      "L: 8\n",
      "9\n",
      "L: 9\n",
      "10\n",
      "L: 10\n",
      "11\n",
      "L: 11\n",
      "12\n",
      "L: 12\n",
      "13\n",
      "L: 13\n",
      "14\n",
      "L: 14\n",
      "15\n",
      "L: 15\n",
      "16\n",
      "L: 16\n",
      "17\n",
      "L: 17\n",
      "18\n",
      "L: 18\n",
      "19\n",
      "L: 19\n",
      "20\n",
      "L: 20\n",
      "21\n",
      "L: 21\n",
      "22\n",
      "L: 22\n",
      "23\n",
      "L: 23\n",
      "24\n",
      "L: 24\n",
      "25\n",
      "L: 25\n",
      "26\n",
      "L: 26\n",
      "27\n",
      "L: 27\n",
      "28\n",
      "L: 28\n",
      "29\n",
      "L: 29\n",
      "30\n",
      "L: 30\n",
      "31\n",
      "L: 31\n",
      "32\n",
      "L: 32\n",
      "33\n",
      "L: 33\n",
      "34\n",
      "L: 34\n",
      "35\n",
      "L: 35\n",
      "36\n",
      "L: 36\n",
      "37\n",
      "L: 37\n",
      "38\n",
      "L: 38\n",
      "39\n",
      "L: 39\n",
      "40\n",
      "L: 40\n",
      "41\n",
      "L: 41\n",
      "42\n",
      "L: 42\n",
      "43\n",
      "L: 43\n",
      "44\n",
      "L: 44\n",
      "45\n",
      "L: 45\n",
      "46\n",
      "L: 46\n",
      "47\n",
      "L: 47\n",
      "48\n",
      "L: 48\n",
      "49\n",
      "L: 49\n",
      "50\n",
      "L: 50\n",
      "51\n",
      "L: 51\n"
     ]
    }
   ],
   "source": [
    "# 1. build dependency graph for resnet18\n",
    "model = copy.deepcopy(mobilenet_v3)\n",
    "\n",
    "correct = []\n",
    "example_inputs = torch.randn(1,3,32,32)\n",
    "for layer_ind in range(len(conv_layers)):\n",
    "    layer = conv_layers[layer_ind]\n",
    "    DG = tp.DependencyGraph().build_dependency(model, example_inputs=example_inputs)\n",
    "\n",
    "    # 2. Select some channels to prune. Here we prune the channels indexed by [2, 6, 9].\n",
    "    pruning_idxs = pruning_idxs=[2, 6, 9]\n",
    "    try:\n",
    "        print(layer_ind)\n",
    "        pruning_group = DG.get_pruning_group( layer, tp.prune_conv_out_channels, idxs=pruning_idxs )\n",
    "\n",
    "        # 3. prune all grouped layer that is coupled with model.conv1\n",
    "        if DG.check_pruning_group(pruning_group):\n",
    "            pruning_group.prune()\n",
    "        correct.append(layer_ind)\n",
    "        \n",
    "    except:\n",
    "        print(\"L:\",layer_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3dd2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ba9849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "from collections import defaultdict, OrderedDict\n",
    "import ipdb\n",
    "import torch_pruning as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4659c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_mapping(model):\n",
    "    \n",
    "    def get_all_layers(model, parent_name=''):\n",
    "        layers = []\n",
    "\n",
    "        def reformat_layer_name(str_data):\n",
    "            try:\n",
    "                split_data = str_data.split('.')\n",
    "                for ind in range(len(split_data)):\n",
    "                    data = split_data[ind]\n",
    "                    if (data.isdigit()):\n",
    "                        split_data[ind] = \"[\" + data + \"]\"\n",
    "                final_string = '.'.join(split_data)\n",
    "\n",
    "                iters_a = re.finditer(r'[a-zA-Z]\\.\\[', final_string)\n",
    "                indices = [m.start(0) + 1 for m in iters_a]\n",
    "                iters = re.finditer(r'\\]\\.\\[', final_string)\n",
    "                indices.extend([m.start(0) + 1 for m in iters])\n",
    "\n",
    "                final_string = list(final_string)\n",
    "                final_string = [final_string[i] for i in range(len(final_string)) if i not in indices]\n",
    "\n",
    "                str_data = ''.join(final_string)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            return str_data\n",
    "\n",
    "        for name, module in model.named_children():\n",
    "            full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "            test_name = \"model.\" + full_name\n",
    "            try:\n",
    "                eval(test_name)\n",
    "                layers.append((full_name, module))\n",
    "            except:\n",
    "                layers.append((reformat_layer_name(full_name), module))\n",
    "            if isinstance(module, nn.Module):\n",
    "                layers.extend(get_all_layers(module, parent_name=full_name))\n",
    "        return layers\n",
    "    all_layers = get_all_layers(model)\n",
    "    model_summary = summary_string_fixed(model, all_layers, (3, 64, 64), model_name='model')  # , device=\"cuda\")\n",
    "\n",
    "    name_type_shape = []\n",
    "    for key in model_summary.keys():\n",
    "        data = model_summary[key]\n",
    "        if (\"weight_shape\" in data.keys()):\n",
    "            name_type_shape.append([key, data['type'], [data['weight_shape']]])\n",
    "        #     else:\n",
    "    #         name_type_shape.append([key, data['type'], 0 ])\n",
    "    name_type_shape = np.asarray(name_type_shape)\n",
    "\n",
    "    name_list = name_type_shape[:, 0]\n",
    "\n",
    "    r_name_list = np.asarray(name_list)\n",
    "    random_picks = np.random.randint(0, len(r_name_list), 10)\n",
    "    test_name_list = r_name_list[random_picks]\n",
    "    eval_hit = False\n",
    "    for layer in test_name_list:\n",
    "        try:\n",
    "            eval(layer)\n",
    "\n",
    "        except:\n",
    "            eval_hit = True\n",
    "            break\n",
    "    if (eval_hit):\n",
    "        fixed_name_list = name_fixer(r_name_list)\n",
    "        name_type_shape[:, 0] = fixed_name_list\n",
    "\n",
    "    layer_types = name_type_shape[:, 1]\n",
    "    layer_shapes = name_type_shape[:, 2]\n",
    "\n",
    "\n",
    "    return name_type_shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def summary_string_fixed(model, all_layers, input_size, model_name=None, batch_size=-1, dtypes=None):\n",
    "    if dtypes is None:\n",
    "        dtypes = [torch.FloatTensor] * len(input_size)\n",
    "        \n",
    "    def name_fixer(names):\n",
    "\n",
    "        return_list = []\n",
    "        for string in names:\n",
    "            matches = re.finditer(r'\\.\\[(\\d+)\\]', string)\n",
    "            pop_list = [m.start(0) for m in matches]\n",
    "            pop_list.sort(reverse=True)\n",
    "            if len(pop_list) > 0:\n",
    "                string = list(string)\n",
    "                for pop_id in pop_list:\n",
    "                    string.pop(pop_id)\n",
    "                string = ''.join(string)\n",
    "            return_list.append(string)\n",
    "        return return_list\n",
    "\n",
    "    def register_hook(module, module_idx):\n",
    "        def hook(module, input, output):\n",
    "            nonlocal module_idx\n",
    "            m_key = all_layers[module_idx][0]\n",
    "            m_key = model_name + \".\" + m_key\n",
    "\n",
    "            try:\n",
    "                eval(m_key)\n",
    "            except:\n",
    "                m_key = name_fixer([m_key])[0]\n",
    "\n",
    "            summary[m_key] = OrderedDict()\n",
    "            summary[m_key][\"type\"] = str(type(module)).split('.')[-1][:-2]\n",
    "            summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "            summary[m_key][\"input_shape\"][0] = batch_size\n",
    "\n",
    "            if isinstance(output, (list, tuple)):\n",
    "                summary[m_key][\"output_shape\"] = [\n",
    "                    [-1] + list(o.size())[1:] for o in output\n",
    "                ]\n",
    "            else:\n",
    "                summary[m_key][\"output_shape\"] = list(output.size())\n",
    "                summary[m_key][\"output_shape\"][0] = batch_size\n",
    "\n",
    "            params = 0\n",
    "            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "                summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
    "                summary[m_key][\"weight_shape\"] = module.weight.shape\n",
    "            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "            summary[m_key][\"nb_params\"] = params\n",
    "\n",
    "        if (\n",
    "                not isinstance(module, nn.Sequential)\n",
    "                and not isinstance(module, nn.ModuleList)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "    if isinstance(input_size, tuple):\n",
    "        input_size = [input_size]\n",
    "\n",
    "    x = [torch.rand(2, *in_size).type(dtype)\n",
    "         for in_size, dtype in zip(input_size, dtypes)]\n",
    "\n",
    "    summary = OrderedDict()\n",
    "    hooks = []\n",
    "\n",
    "    for module_idx, (layer_name, module) in enumerate(all_layers):\n",
    "        register_hook(module, module_idx)\n",
    "\n",
    "    model(*x)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d97d3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VGG(nn.Module):\n",
    "    ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        counts = defaultdict(int)\n",
    "\n",
    "        def add(name: str, layer: nn.Module) -> None:\n",
    "            layers.append((f\"{name}{counts[name]}\", layer))\n",
    "            counts[name] += 1\n",
    "\n",
    "        in_channels = 3\n",
    "        for x in self.ARCH:\n",
    "            if x != 'M':\n",
    "                # conv-bn-relu\n",
    "                add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n",
    "                add(\"bn\", nn.BatchNorm2d(x))\n",
    "                add(\"relu\", nn.ReLU(True))\n",
    "                in_channels = x\n",
    "            else:\n",
    "                # maxpool\n",
    "                add(\"pool\", nn.MaxPool2d(2))\n",
    "\n",
    "        self.backbone = nn.Sequential(OrderedDict(layers))\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        # avgpool: [N, 512, 2, 2] => [N, 512]\n",
    "        x = x.mean([2, 3])\n",
    "\n",
    "        # classifier: [N, 512] => [N, 10]\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23105eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VGG()\n",
    "parent_name = ''\n",
    "layers= []\n",
    "\n",
    "def summary_string_fixed(model, all_layers, input_size, model_name=None, batch_size=-1, dtypes=None):\n",
    "    if dtypes is None:\n",
    "        dtypes = [torch.FloatTensor] * len(input_size)\n",
    "        \n",
    "    def name_fixer(names):\n",
    "\n",
    "        return_list = []\n",
    "        for string in names:\n",
    "            matches = re.finditer(r'\\.\\[(\\d+)\\]', string)\n",
    "            pop_list = [m.start(0) for m in matches]\n",
    "            pop_list.sort(reverse=True)\n",
    "            if len(pop_list) > 0:\n",
    "                string = list(string)\n",
    "                for pop_id in pop_list:\n",
    "                    string.pop(pop_id)\n",
    "                string = ''.join(string)\n",
    "            return_list.append(string)\n",
    "        return return_list\n",
    "\n",
    "    def register_hook(module, module_idx):\n",
    "        def hook(module, input, output):\n",
    "            nonlocal module_idx\n",
    "            m_key = all_layers[module_idx][0]\n",
    "            m_key = model_name + \".\" + m_key\n",
    "\n",
    "            try:\n",
    "                eval(m_key)\n",
    "            except:\n",
    "                m_key = name_fixer([m_key])[0]\n",
    "\n",
    "         \n",
    "            summary[m_key] = str(type(module)).split('.')[-1][:-2]\n",
    "            \n",
    "\n",
    "        if (\n",
    "                not isinstance(module, nn.Sequential)\n",
    "                and not isinstance(module, nn.ModuleList)\n",
    "                and not isinstance(module, torchvision.ops.misc.SqueezeExcitation)\n",
    "                and not isinstance(module, torchvision.models.mobilenetv3.InvertedResidual)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "    if isinstance(input_size, tuple):\n",
    "        input_size = [input_size]\n",
    "\n",
    "    x = [torch.rand(2, *in_size).type(dtype)\n",
    "         for in_size, dtype in zip(input_size, dtypes)]\n",
    "\n",
    "    summary = OrderedDict()\n",
    "    hooks = []\n",
    "\n",
    "    for module_idx, (layer_name, module) in enumerate(all_layers):\n",
    "        register_hook(module, module_idx)\n",
    "\n",
    "    model(*x)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def get_all_layers(model, parent_name=''):\n",
    "        layers = []\n",
    "\n",
    "        def reformat_layer_name(str_data):\n",
    "            try:\n",
    "                split_data = str_data.split('.')\n",
    "                for ind in range(len(split_data)):\n",
    "                    data = split_data[ind]\n",
    "                    if (data.isdigit()):\n",
    "                        split_data[ind] = \"[\" + data + \"]\"\n",
    "                final_string = '.'.join(split_data)\n",
    "\n",
    "                iters_a = re.finditer(r'[a-zA-Z]\\.\\[', final_string)\n",
    "                indices = [m.start(0) + 1 for m in iters_a]\n",
    "                iters = re.finditer(r'\\]\\.\\[', final_string)\n",
    "                indices.extend([m.start(0) + 1 for m in iters])\n",
    "\n",
    "                final_string = list(final_string)\n",
    "                final_string = [final_string[i] for i in range(len(final_string)) if i not in indices]\n",
    "\n",
    "                str_data = ''.join(final_string)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            return str_data\n",
    "\n",
    "        for name, module in model.named_children():\n",
    "            full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "            test_name = \"model.\" + full_name\n",
    "            try:\n",
    "                eval(test_name)\n",
    "                layers.append((full_name, module))\n",
    "            except:\n",
    "                layers.append((reformat_layer_name(full_name), module))\n",
    "            if isinstance(module, nn.Module):\n",
    "                layers.extend(get_all_layers(module, parent_name=full_name))\n",
    "        return layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0679848b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/sathya/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/Users/sathya/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sathya/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in /Users/sathya/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/Users/sathya/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "vgg = VGG()\n",
    "mobilenet_v2 = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "mobilenet_v3 = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v3_small', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d1ca7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_importance(layer, sparsity):\n",
    "\n",
    "    def get_input_channel_importance(weight):\n",
    "        importances = []\n",
    "        for o_c in range(weight.shape[0]):\n",
    "            channel_weight = weight.detach()[o_c,:]\n",
    "            importance = torch.norm(channel_weight)\n",
    "            importances.append(importance.view(1))\n",
    "        return torch.cat(importances)\n",
    "\n",
    "    sorted_indices = torch.argsort(get_input_channel_importance(layer.weight), descending=True)\n",
    "    n_keep = int(round(len(sorted_indices) * (1.0 - sparsity)))\n",
    "    indices_to_remove = sorted_indices[n_keep:]\n",
    "    \n",
    "    return indices_to_remove\n",
    "\n",
    "\n",
    "import copy\n",
    "model = copy.deepcopy(mobilenet_v3)\n",
    "all_layers = get_all_layers(model)\n",
    "model_summary = summary_string_fixed(model, all_layers, (3, 64, 64), model_name='model')\n",
    "conv_layer = []\n",
    "for key, value in model_summary.items():\n",
    "    if(value == 'Conv2d'):\n",
    "        conv_layer.append(key)\n",
    "grouped = []\n",
    "\n",
    "for i in range(len(conv_layer)):\n",
    "    model = copy.deepcopy(mobilenet_v3)\n",
    "    \n",
    "    layer = eval(conv_layer[i])\n",
    "    indices_to_remove = get_importance(layer, sparsity=0.10)\n",
    "\n",
    "\n",
    "    example_inputs = torch.randn(1,3,32,32)\n",
    "\n",
    "\n",
    "    DG = tp.DependencyGraph().build_dependency(model, example_inputs=example_inputs)\n",
    "\n",
    "    pruning_group = DG.get_pruning_group( layer, tp.prune_conv_out_channels, idxs=indices_to_remove )\n",
    "\n",
    "    if DG.check_pruning_group(pruning_group):\n",
    "        pruning_group.prune()\n",
    "    try:\n",
    "        model(example_inputs)\n",
    "    except:\n",
    "        grouped.append(i)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7cd28f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 11,\n",
       " 12,\n",
       " 14,\n",
       " 16,\n",
       " 17,\n",
       " 19,\n",
       " 21,\n",
       " 22,\n",
       " 24,\n",
       " 26,\n",
       " 27,\n",
       " 29,\n",
       " 31,\n",
       " 32,\n",
       " 34,\n",
       " 36,\n",
       " 37,\n",
       " 39,\n",
       " 41,\n",
       " 42,\n",
       " 44,\n",
       " 46,\n",
       " 47,\n",
       " 49]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98eaa2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "working = []\n",
    "for i in range(len(conv_layer)):\n",
    "    if i not in grouped:\n",
    "        working.append(i)\n",
    "# working.append(grouped)\n",
    "# print(working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ef91ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV3(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): Conv2dNormActivation(\n",
       "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
       "    (1): Hardswish()\n",
       "    (2): Dropout(p=0.2, inplace=True)\n",
       "    (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobilenet_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea06fe59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 16, 16]             432\n",
      "       BatchNorm2d-2           [-1, 16, 16, 16]              32\n",
      "         Hardswish-3           [-1, 16, 16, 16]               0\n",
      "            Conv2d-4             [-1, 16, 8, 8]             144\n",
      "       BatchNorm2d-5             [-1, 16, 8, 8]              32\n",
      "              ReLU-6             [-1, 16, 8, 8]               0\n",
      " AdaptiveAvgPool2d-7             [-1, 16, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]             136\n",
      "              ReLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10             [-1, 16, 1, 1]             144\n",
      "      Hardsigmoid-11             [-1, 16, 1, 1]               0\n",
      "SqueezeExcitation-12             [-1, 16, 8, 8]               0\n",
      "           Conv2d-13             [-1, 16, 8, 8]             256\n",
      "      BatchNorm2d-14             [-1, 16, 8, 8]              32\n",
      " InvertedResidual-15             [-1, 16, 8, 8]               0\n",
      "           Conv2d-16             [-1, 72, 8, 8]           1,152\n",
      "      BatchNorm2d-17             [-1, 72, 8, 8]             144\n",
      "             ReLU-18             [-1, 72, 8, 8]               0\n",
      "           Conv2d-19             [-1, 72, 4, 4]             648\n",
      "      BatchNorm2d-20             [-1, 72, 4, 4]             144\n",
      "             ReLU-21             [-1, 72, 4, 4]               0\n",
      "           Conv2d-22             [-1, 24, 4, 4]           1,728\n",
      "      BatchNorm2d-23             [-1, 24, 4, 4]              48\n",
      " InvertedResidual-24             [-1, 24, 4, 4]               0\n",
      "           Conv2d-25             [-1, 88, 4, 4]           2,112\n",
      "      BatchNorm2d-26             [-1, 88, 4, 4]             176\n",
      "             ReLU-27             [-1, 88, 4, 4]               0\n",
      "           Conv2d-28             [-1, 88, 4, 4]             792\n",
      "      BatchNorm2d-29             [-1, 88, 4, 4]             176\n",
      "             ReLU-30             [-1, 88, 4, 4]               0\n",
      "           Conv2d-31             [-1, 24, 4, 4]           2,112\n",
      "      BatchNorm2d-32             [-1, 24, 4, 4]              48\n",
      " InvertedResidual-33             [-1, 24, 4, 4]               0\n",
      "           Conv2d-34             [-1, 96, 4, 4]           2,304\n",
      "      BatchNorm2d-35             [-1, 96, 4, 4]             192\n",
      "        Hardswish-36             [-1, 96, 4, 4]               0\n",
      "           Conv2d-37             [-1, 96, 2, 2]           2,400\n",
      "      BatchNorm2d-38             [-1, 96, 2, 2]             192\n",
      "        Hardswish-39             [-1, 96, 2, 2]               0\n",
      "AdaptiveAvgPool2d-40             [-1, 96, 1, 1]               0\n",
      "           Conv2d-41             [-1, 24, 1, 1]           2,328\n",
      "             ReLU-42             [-1, 24, 1, 1]               0\n",
      "           Conv2d-43             [-1, 96, 1, 1]           2,400\n",
      "      Hardsigmoid-44             [-1, 96, 1, 1]               0\n",
      "SqueezeExcitation-45             [-1, 96, 2, 2]               0\n",
      "           Conv2d-46             [-1, 40, 2, 2]           3,840\n",
      "      BatchNorm2d-47             [-1, 40, 2, 2]              80\n",
      " InvertedResidual-48             [-1, 40, 2, 2]               0\n",
      "           Conv2d-49            [-1, 240, 2, 2]           9,600\n",
      "      BatchNorm2d-50            [-1, 240, 2, 2]             480\n",
      "        Hardswish-51            [-1, 240, 2, 2]               0\n",
      "           Conv2d-52            [-1, 240, 2, 2]           6,000\n",
      "      BatchNorm2d-53            [-1, 240, 2, 2]             480\n",
      "        Hardswish-54            [-1, 240, 2, 2]               0\n",
      "AdaptiveAvgPool2d-55            [-1, 240, 1, 1]               0\n",
      "           Conv2d-56             [-1, 64, 1, 1]          15,424\n",
      "             ReLU-57             [-1, 64, 1, 1]               0\n",
      "           Conv2d-58            [-1, 240, 1, 1]          15,600\n",
      "      Hardsigmoid-59            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-60            [-1, 240, 2, 2]               0\n",
      "           Conv2d-61             [-1, 40, 2, 2]           9,600\n",
      "      BatchNorm2d-62             [-1, 40, 2, 2]              80\n",
      " InvertedResidual-63             [-1, 40, 2, 2]               0\n",
      "           Conv2d-64            [-1, 240, 2, 2]           9,600\n",
      "      BatchNorm2d-65            [-1, 240, 2, 2]             480\n",
      "        Hardswish-66            [-1, 240, 2, 2]               0\n",
      "           Conv2d-67            [-1, 240, 2, 2]           6,000\n",
      "      BatchNorm2d-68            [-1, 240, 2, 2]             480\n",
      "        Hardswish-69            [-1, 240, 2, 2]               0\n",
      "AdaptiveAvgPool2d-70            [-1, 240, 1, 1]               0\n",
      "           Conv2d-71             [-1, 64, 1, 1]          15,424\n",
      "             ReLU-72             [-1, 64, 1, 1]               0\n",
      "           Conv2d-73            [-1, 240, 1, 1]          15,600\n",
      "      Hardsigmoid-74            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-75            [-1, 240, 2, 2]               0\n",
      "           Conv2d-76             [-1, 40, 2, 2]           9,600\n",
      "      BatchNorm2d-77             [-1, 40, 2, 2]              80\n",
      " InvertedResidual-78             [-1, 40, 2, 2]               0\n",
      "           Conv2d-79            [-1, 120, 2, 2]           4,800\n",
      "      BatchNorm2d-80            [-1, 120, 2, 2]             240\n",
      "        Hardswish-81            [-1, 120, 2, 2]               0\n",
      "           Conv2d-82            [-1, 120, 2, 2]           3,000\n",
      "      BatchNorm2d-83            [-1, 120, 2, 2]             240\n",
      "        Hardswish-84            [-1, 120, 2, 2]               0\n",
      "AdaptiveAvgPool2d-85            [-1, 120, 1, 1]               0\n",
      "           Conv2d-86             [-1, 32, 1, 1]           3,872\n",
      "             ReLU-87             [-1, 32, 1, 1]               0\n",
      "           Conv2d-88            [-1, 120, 1, 1]           3,960\n",
      "      Hardsigmoid-89            [-1, 120, 1, 1]               0\n",
      "SqueezeExcitation-90            [-1, 120, 2, 2]               0\n",
      "           Conv2d-91             [-1, 48, 2, 2]           5,760\n",
      "      BatchNorm2d-92             [-1, 48, 2, 2]              96\n",
      " InvertedResidual-93             [-1, 48, 2, 2]               0\n",
      "           Conv2d-94            [-1, 144, 2, 2]           6,912\n",
      "      BatchNorm2d-95            [-1, 144, 2, 2]             288\n",
      "        Hardswish-96            [-1, 144, 2, 2]               0\n",
      "           Conv2d-97            [-1, 144, 2, 2]           3,600\n",
      "      BatchNorm2d-98            [-1, 144, 2, 2]             288\n",
      "        Hardswish-99            [-1, 144, 2, 2]               0\n",
      "AdaptiveAvgPool2d-100            [-1, 144, 1, 1]               0\n",
      "          Conv2d-101             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-102             [-1, 40, 1, 1]               0\n",
      "          Conv2d-103            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-104            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-105            [-1, 144, 2, 2]               0\n",
      "          Conv2d-106             [-1, 48, 2, 2]           6,912\n",
      "     BatchNorm2d-107             [-1, 48, 2, 2]              96\n",
      "InvertedResidual-108             [-1, 48, 2, 2]               0\n",
      "          Conv2d-109            [-1, 288, 2, 2]          13,824\n",
      "     BatchNorm2d-110            [-1, 288, 2, 2]             576\n",
      "       Hardswish-111            [-1, 288, 2, 2]               0\n",
      "          Conv2d-112            [-1, 288, 1, 1]           7,200\n",
      "     BatchNorm2d-113            [-1, 288, 1, 1]             576\n",
      "       Hardswish-114            [-1, 288, 1, 1]               0\n",
      "AdaptiveAvgPool2d-115            [-1, 288, 1, 1]               0\n",
      "          Conv2d-116             [-1, 72, 1, 1]          20,808\n",
      "            ReLU-117             [-1, 72, 1, 1]               0\n",
      "          Conv2d-118            [-1, 288, 1, 1]          21,024\n",
      "     Hardsigmoid-119            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-120            [-1, 288, 1, 1]               0\n",
      "          Conv2d-121             [-1, 96, 1, 1]          27,648\n",
      "     BatchNorm2d-122             [-1, 96, 1, 1]             192\n",
      "InvertedResidual-123             [-1, 96, 1, 1]               0\n",
      "          Conv2d-124            [-1, 576, 1, 1]          55,296\n",
      "     BatchNorm2d-125            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-126            [-1, 576, 1, 1]               0\n",
      "          Conv2d-127            [-1, 576, 1, 1]          14,400\n",
      "     BatchNorm2d-128            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-129            [-1, 576, 1, 1]               0\n",
      "AdaptiveAvgPool2d-130            [-1, 576, 1, 1]               0\n",
      "          Conv2d-131            [-1, 144, 1, 1]          83,088\n",
      "            ReLU-132            [-1, 144, 1, 1]               0\n",
      "          Conv2d-133            [-1, 576, 1, 1]          83,520\n",
      "     Hardsigmoid-134            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-135            [-1, 576, 1, 1]               0\n",
      "          Conv2d-136             [-1, 96, 1, 1]          55,296\n",
      "     BatchNorm2d-137             [-1, 96, 1, 1]             192\n",
      "InvertedResidual-138             [-1, 96, 1, 1]               0\n",
      "          Conv2d-139            [-1, 576, 1, 1]          55,296\n",
      "     BatchNorm2d-140            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-141            [-1, 576, 1, 1]               0\n",
      "          Conv2d-142            [-1, 576, 1, 1]          14,400\n",
      "     BatchNorm2d-143            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-144            [-1, 576, 1, 1]               0\n",
      "AdaptiveAvgPool2d-145            [-1, 576, 1, 1]               0\n",
      "          Conv2d-146            [-1, 144, 1, 1]          83,088\n",
      "            ReLU-147            [-1, 144, 1, 1]               0\n",
      "          Conv2d-148            [-1, 576, 1, 1]          83,520\n",
      "     Hardsigmoid-149            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-150            [-1, 576, 1, 1]               0\n",
      "          Conv2d-151             [-1, 96, 1, 1]          55,296\n",
      "     BatchNorm2d-152             [-1, 96, 1, 1]             192\n",
      "InvertedResidual-153             [-1, 96, 1, 1]               0\n",
      "          Conv2d-154            [-1, 576, 1, 1]          55,296\n",
      "     BatchNorm2d-155            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-156            [-1, 576, 1, 1]               0\n",
      "AdaptiveAvgPool2d-157            [-1, 576, 1, 1]               0\n",
      "          Linear-158                 [-1, 1024]         590,848\n",
      "       Hardswish-159                 [-1, 1024]               0\n",
      "         Dropout-160                 [-1, 1024]               0\n",
      "          Linear-161                 [-1, 1000]       1,025,000\n",
      "================================================================\n",
      "Total params: 2,542,856\n",
      "Trainable params: 2,542,856\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.80\n",
      "Params size (MB): 9.70\n",
      "Estimated Total Size (MB): 10.51\n",
      "----------------------------------------------------------------\n",
      "I: 0\n",
      "model.features[1].block[1].fc1 Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1)) tensor([6, 0, 7, 1, 3, 4])\n",
      "++++++++\n",
      "I: 1\n",
      "model.features[1].block[2][0] Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False) tensor([ 7,  9,  3,  4,  1,  6, 15, 12,  8, 13,  5, 14,  0])\n",
      "++++++++\n",
      "I: 2\n",
      "model.features[2].block[2][0] Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False) tensor([ 5,  8,  1, 17,  0,  9,  7, 20,  6, 19, 12,  4,  3, 22, 15, 23,  2, 14,\n",
      "        21])\n",
      "++++++++\n",
      "I: 3\n",
      "model.features[3].block[2][0] Conv2d(88, 5, kernel_size=(1, 1), stride=(1, 1), bias=False) tensor([ 0, 21, 17,  5,  2, 18,  8,  7, 20,  9, 23, 12, 19, 15,  4,  3,  1,  6,\n",
      "        10])\n",
      "++++++++\n",
      "I: 4\n",
      "model.features[4].block[2].fc1 Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1)) tensor([ 9, 20, 14,  5, 23,  7,  8,  1, 17,  0,  2,  6,  3,  4, 12, 22, 16, 11,\n",
      "        18])\n",
      "++++++++\n",
      "I: 5\n",
      "model.features[4].block[3][0] Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False) tensor([34, 39, 23, 30,  4, 27, 24, 19, 20, 10, 32, 35, 28, 29, 21,  9, 33, 37,\n",
      "        31,  7, 22,  6, 26,  3,  2, 13, 15,  0, 17,  8, 18, 25])\n",
      "++++++++\n",
      "I: 6\n",
      "model.features[5].block[2].fc1 Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1)) tensor([16, 28, 63, 20, 35, 62, 56, 53, 60,  7, 54,  3, 59, 49, 29, 39, 26, 25,\n",
      "         9, 24, 11, 47,  0, 37,  5, 40, 31, 51, 61,  8, 15, 44,  6, 58, 32, 21,\n",
      "        17, 23,  2, 19, 36, 45, 41, 48, 42, 55, 33, 10, 34, 52, 22])\n",
      "++++++++\n",
      "I: 7\n",
      "model.features[5].block[3][0] Conv2d(240, 8, kernel_size=(1, 1), stride=(1, 1), bias=False) tensor([14,  7, 39, 20, 15, 26, 21,  0, 25, 34, 11,  6, 32, 35,  2,  8, 29, 28,\n",
      "        18, 19,  5, 36, 13, 16, 24, 23,  3, 37,  9, 31, 10, 33])\n",
      "++++++++\n",
      "I: 8\n",
      "model.features[6].block[2].fc1 Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1)) tensor([22, 10,  1, 32, 19, 37, 55, 58, 44,  8,  2, 52, 33, 39, 27, 43,  0,  4,\n",
      "        36, 29, 46, 28, 18, 49,  9, 25, 16, 35, 47, 12, 45, 11, 51, 34, 13, 14,\n",
      "         3, 61, 42, 59, 60, 26, 50, 17, 40,  6, 20, 23,  5, 57, 62])\n",
      "++++++++\n",
      "I: 9\n",
      "model.features[6].block[3][0] Conv2d(240, 8, kernel_size=(1, 1), stride=(1, 1), bias=False) tensor([ 8, 11, 28, 25,  3, 26,  5, 39, 13,  6,  9, 12, 19, 22, 33, 36,  2, 34,\n",
      "        23, 38, 24, 10, 31, 20, 16, 32, 30, 35, 29, 27,  4, 21])\n",
      "++++++++\n",
      "I: 10\n",
      "model.features[7].block[2].fc1 Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1)) tensor([12, 30, 16,  4,  3, 23, 24, 25, 11,  5, 27,  1, 26,  7,  0,  8, 21,  6,\n",
      "        29, 19, 13, 31, 17, 14, 28,  2])\n",
      "++++++++\n",
      "I: 11\n",
      "model.features[7].block[3][0] Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False) tensor([38, 34, 33, 25, 32,  7, 22, 12, 30, 11, 15, 10, 27,  4, 47,  3,  0, 13,\n",
      "        46, 31, 41, 18, 24, 17, 35, 36,  8, 21, 28,  6, 37, 16, 39, 20, 43,  2,\n",
      "        44, 45])\n",
      "++++++++\n",
      "I: 12\n",
      "model.features[8].block[2].fc1 Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1)) tensor([16, 19, 22, 14, 10,  3, 31,  9, 18, 32,  0, 15, 26, 30, 11,  7, 24, 27,\n",
      "        21, 36, 13,  1,  5, 17, 20, 38, 12, 34, 35, 25,  8, 29])\n",
      "++++++++\n",
      "I: 13\n",
      "model.features[8].block[3][0] Conv2d(144, 10, kernel_size=(1, 1), stride=(1, 1), bias=False) tensor([24,  2, 37, 18, 41, 20, 21, 12, 23, 30, 16, 34,  5, 13, 46, 39,  9, 11,\n",
      "         4, 31, 47, 35,  1, 33, 15, 29, 25, 32, 42,  3,  7, 27, 22, 38, 26, 14,\n",
      "        10, 19])\n",
      "++++++++\n",
      "I: 14\n",
      "model.features[9].block[2].fc1 Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1)) tensor([63,  5, 58, 41, 36, 51,  8, 70, 29, 64, 46,  6, 47, 37, 10, 13, 24, 44,\n",
      "        39, 38, 22, 15, 25, 56, 61, 11, 48, 71, 66, 68, 20, 32,  7,  1, 60, 19,\n",
      "        57, 52, 50, 31, 59, 40, 49, 43, 16, 69, 54, 28, 35,  3, 42, 21, 67,  9,\n",
      "        14, 55, 53, 23])\n",
      "++++++++\n",
      "I: 15\n",
      "model.features[9].block[3][0] Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False) tensor([ 2, 90,  1, 31, 78, 89, 85, 20, 10, 25, 33, 82, 39, 80, 81, 18, 95, 87,\n",
      "        56, 83, 24, 42, 50,  9, 86, 37, 75, 69, 64, 15,  5, 17, 88, 47,  0, 54,\n",
      "        76, 68,  3, 71, 63, 91, 49,  6, 92, 58, 53, 38, 57, 16, 66, 55, 46, 34,\n",
      "        74, 12, 26, 77, 59, 35, 13, 43, 32,  8, 22, 79, 60, 23, 72,  7, 61, 19,\n",
      "        51, 21, 94, 62, 84])\n",
      "++++++++\n",
      "I: 16\n",
      "model.features[10].block[2].fc1 Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1)) tensor([ 79, 116,  94,  10,  69, 104,  42,  57,  89,  67, 140,  13,  20,  54,\n",
      "         53,  86, 103,  80, 113,  82,  22,  48,  84,   8,  23,  30,  52,  85,\n",
      "        126,  44,  51,  28, 142,  43,  81, 129,  12,  76,  29, 105,  15, 141,\n",
      "         11,  88, 137,   5,  64, 135,  16,   2, 127, 132,  55,  58,  50,  90,\n",
      "          7, 118,  75,  96,  61, 123,  39, 107,  40, 119,  62,  74,  63, 108,\n",
      "         70,  66, 101,  24,  56,  92,  25, 130, 131,   4, 143, 124, 117,   0,\n",
      "        100,  38,  37, 121,  49, 112,  83, 111,  19, 138, 106, 114,   6,  36,\n",
      "        120,  68,  72, 122,  77,  47,  27,  33,   9,  35,  99, 109,  31,  98,\n",
      "        115,  71,  60])\n",
      "++++++++\n",
      "I: 17\n",
      "model.features[10].block[3][0] Conv2d(576, 19, kernel_size=(1, 1), stride=(1, 1), bias=False) tensor([76, 38, 61, 37, 65, 64, 68, 16, 79, 33, 53, 23, 25, 95, 49, 34,  7, 18,\n",
      "         0, 48, 50, 88, 91, 13, 78, 51, 41, 83, 11, 46, 35, 86, 72,  5,  3, 55,\n",
      "        30, 47, 29, 56, 36, 82, 67, 28, 87, 69, 80, 14, 42, 58, 45, 92, 15, 93,\n",
      "        63, 57,  2, 81, 12, 90,  4, 17, 89,  9, 52, 59, 43, 10, 70, 26,  1, 73,\n",
      "         6, 44, 20, 85, 75])\n",
      "++++++++\n",
      "I: 18\n",
      "model.features[11].block[2].fc1 Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1)) tensor([137,  45, 110,  77,  51,  71, 112, 138,  21,  17,  98,  49,   4, 106,\n",
      "         32,  31,  10,  56,   5,  12,  24,  89,  85,  63,   2,  75, 130, 125,\n",
      "          1,  78, 115,  53, 140, 133,  92,  34,  76, 136, 113, 104, 128,  94,\n",
      "        139, 119,  59,  27,  93,  48,  65, 129,  68,  14,  88,  52,   7,  46,\n",
      "         82,  90,  47,  42,   3, 101, 108,  29,  95,  84, 111,  33,  18,  64,\n",
      "         16, 131,  30, 122,  41,  86,  62,   6, 126,  23,  81,  60,  67, 102,\n",
      "         55,  20,  36,  87,  58, 100, 127, 120,  11,  91,  26,  44,  61, 103,\n",
      "          8, 107,  15,  50, 124,  80, 117,  96, 141,  39,  37,  99,  57,   0,\n",
      "         97,  74,  54])\n",
      "++++++++\n",
      "I: 19\n",
      "model.features[11].block[3][0] Conv2d(576, 19, kernel_size=(1, 1), stride=(1, 1), bias=False) tensor([54, 18, 31, 95, 91, 37, 28, 33,  2, 34, 41,  0, 63, 55, 50, 67, 94, 13,\n",
      "        49, 64, 70, 76, 79, 39,  5, 22, 82,  9, 45, 48, 93, 92,  7, 35, 65, 86,\n",
      "        56, 14, 36, 25, 68, 57, 58,  3, 88, 81, 46, 73, 42, 51, 29, 80,  6, 90,\n",
      "        47, 52, 44, 12, 30, 89, 72, 83, 17, 43, 11, 10, 78, 87, 15, 69, 20, 59,\n",
      "        85, 26,  4,  1, 75])\n",
      "++++++++\n",
      "I: 20\n",
      "model.features[12][0] Conv2d(19, 576, kernel_size=(1, 1), stride=(1, 1), bias=False) tensor([  2, 552, 277,  81, 309, 181,  95, 176, 532, 286, 226,  47, 326,  73,\n",
      "        136,  58,  21, 508,  13,  35, 553, 247, 154, 527, 358, 210, 292, 161,\n",
      "        144,  84, 188, 218, 356, 230, 170, 374,  19, 299,  39, 141, 514,  91,\n",
      "        471, 295, 540, 432, 197, 447, 252, 105, 384, 393, 193, 469, 249, 494,\n",
      "        311, 534, 117, 341, 198, 561, 231, 369, 162, 128, 208, 279,  25, 437,\n",
      "        120, 536,  51,  49, 482, 330, 194, 125,  30, 443, 550, 457,  71, 511,\n",
      "        445, 521, 164, 260, 185, 300,  15, 291, 441, 281, 234, 345, 431, 204,\n",
      "        102, 496, 200, 522, 492, 560, 111, 344,  42, 270,  78, 364, 405, 555,\n",
      "        539, 575, 360, 196, 253, 223, 483, 390,  97,  79, 538,  17, 222, 408,\n",
      "        297, 187, 430, 255, 107, 195, 206, 265, 486, 523, 175, 379, 422, 313,\n",
      "        518, 251,  89, 242, 490, 425, 259, 403, 335, 458, 433, 325, 423, 108,\n",
      "        219,  87, 235, 354, 399,  85, 383, 572, 474,  80,   8, 248, 183, 324,\n",
      "        363,  55, 132, 412,   3, 449, 535, 512, 244,  10, 301, 127,  56, 376,\n",
      "        150, 103, 258, 400,  53, 112, 503, 546, 220, 182,   0,  23, 115,   4,\n",
      "        138, 211, 454, 171, 487, 304, 446, 169, 191, 444, 331, 203, 524,  22,\n",
      "        459, 303,   6,  68, 427, 224, 190, 488, 365, 184, 267, 343, 416, 146,\n",
      "        294, 563,  96, 461, 122, 501, 554,  27, 337, 419, 465, 475, 367, 542,\n",
      "        157, 506, 207, 436, 268, 569,  76, 510, 139, 377, 468,  34, 305, 574,\n",
      "        347, 158, 114, 232,   5,  43, 274, 109, 298, 202, 316, 371,  61,  36,\n",
      "         92,  69, 489, 315, 451, 318, 467, 165, 463, 573,  11, 372, 409, 361,\n",
      "        380, 131, 351, 424, 332, 245, 541, 551, 186, 148, 357, 417, 177,  82,\n",
      "        381, 368, 124, 528, 153, 480, 570, 402, 462, 263, 296, 340,  20, 507,\n",
      "        396, 520, 434, 213, 370,  88, 472, 307, 339, 389, 450, 516, 334, 567,\n",
      "        453, 478, 246, 121, 243,  60, 126, 397,  66,  65, 149, 571, 152, 493,\n",
      "        217, 240, 327, 216, 557, 278, 159, 499, 438, 156, 241, 118, 562, 155,\n",
      "         37, 228,  14, 502,  29, 272,  16, 167, 302,  70,  75, 266,  32, 391,\n",
      "        491, 388, 116,  52, 477, 129,  86, 533, 442,  44, 104, 352, 504, 289,\n",
      "        262, 113, 531, 227, 322, 145, 287, 366,  63, 199, 310, 414, 407, 565,\n",
      "        237, 209, 163, 564, 306, 338,  99, 320, 205, 543,  90,  46, 421, 426,\n",
      "        269, 556, 349,  98, 355, 189,  72,  64, 151, 130,  77, 236, 254, 271,\n",
      "        537, 123, 386, 282, 143, 160, 290, 137, 439, 448,  48, 464, 250, 566,\n",
      "        395, 312, 348, 233, 387, 333,  28,  94, 517,  24, 280, 180, 101, 428,\n",
      "        285, 485, 229,  31, 275, 382, 495, 212, 106,  74, 261,   9, 173])\n",
      "++++++++\n"
     ]
    }
   ],
   "source": [
    "model = copy.deepcopy(mobilenet_v3)\n",
    "DG = tp.DependencyGraph().build_dependency(model, example_inputs=example_inputs)\n",
    "summary(model,(3,32,32))\n",
    "for i in range(len(working)):\n",
    "    print(\"I:\",i)\n",
    "    \n",
    "    layer = eval(conv_layer[working[i]])\n",
    "    indices_to_remove = get_importance(layer, sparsity=0.80)\n",
    "    print(conv_layer[working[i]],layer, indices_to_remove)\n",
    "\n",
    "    example_inputs = torch.randn(1,3,32,32)\n",
    "\n",
    "    pruning_group = DG.get_pruning_group(layer, tp.prune_conv_out_channels, idxs=indices_to_remove )\n",
    "\n",
    "    if DG.check_pruning_group(pruning_group):\n",
    "        pruning_group.prune()\n",
    "    print(\"++++++++\")\n",
    "#     macs, nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "#     print(macs, nparams)\n",
    "#     break\n",
    "pruned_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "24c4958f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 16, 16]             432\n",
      "       BatchNorm2d-2           [-1, 16, 16, 16]              32\n",
      "         Hardswish-3           [-1, 16, 16, 16]               0\n",
      "            Conv2d-4             [-1, 16, 8, 8]             144\n",
      "       BatchNorm2d-5             [-1, 16, 8, 8]              32\n",
      "              ReLU-6             [-1, 16, 8, 8]               0\n",
      " AdaptiveAvgPool2d-7             [-1, 16, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]             136\n",
      "              ReLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10             [-1, 16, 1, 1]             144\n",
      "      Hardsigmoid-11             [-1, 16, 1, 1]               0\n",
      "SqueezeExcitation-12             [-1, 16, 8, 8]               0\n",
      "           Conv2d-13             [-1, 16, 8, 8]             256\n",
      "      BatchNorm2d-14             [-1, 16, 8, 8]              32\n",
      " InvertedResidual-15             [-1, 16, 8, 8]               0\n",
      "           Conv2d-16             [-1, 72, 8, 8]           1,152\n",
      "      BatchNorm2d-17             [-1, 72, 8, 8]             144\n",
      "             ReLU-18             [-1, 72, 8, 8]               0\n",
      "           Conv2d-19             [-1, 72, 4, 4]             648\n",
      "      BatchNorm2d-20             [-1, 72, 4, 4]             144\n",
      "             ReLU-21             [-1, 72, 4, 4]               0\n",
      "           Conv2d-22             [-1, 24, 4, 4]           1,728\n",
      "      BatchNorm2d-23             [-1, 24, 4, 4]              48\n",
      " InvertedResidual-24             [-1, 24, 4, 4]               0\n",
      "           Conv2d-25             [-1, 88, 4, 4]           2,112\n",
      "      BatchNorm2d-26             [-1, 88, 4, 4]             176\n",
      "             ReLU-27             [-1, 88, 4, 4]               0\n",
      "           Conv2d-28             [-1, 88, 4, 4]             792\n",
      "      BatchNorm2d-29             [-1, 88, 4, 4]             176\n",
      "             ReLU-30             [-1, 88, 4, 4]               0\n",
      "           Conv2d-31             [-1, 24, 4, 4]           2,112\n",
      "      BatchNorm2d-32             [-1, 24, 4, 4]              48\n",
      " InvertedResidual-33             [-1, 24, 4, 4]               0\n",
      "           Conv2d-34             [-1, 96, 4, 4]           2,304\n",
      "      BatchNorm2d-35             [-1, 96, 4, 4]             192\n",
      "        Hardswish-36             [-1, 96, 4, 4]               0\n",
      "           Conv2d-37             [-1, 96, 2, 2]           2,400\n",
      "      BatchNorm2d-38             [-1, 96, 2, 2]             192\n",
      "        Hardswish-39             [-1, 96, 2, 2]               0\n",
      "AdaptiveAvgPool2d-40             [-1, 96, 1, 1]               0\n",
      "           Conv2d-41             [-1, 24, 1, 1]           2,328\n",
      "             ReLU-42             [-1, 24, 1, 1]               0\n",
      "           Conv2d-43             [-1, 96, 1, 1]           2,400\n",
      "      Hardsigmoid-44             [-1, 96, 1, 1]               0\n",
      "SqueezeExcitation-45             [-1, 96, 2, 2]               0\n",
      "           Conv2d-46             [-1, 40, 2, 2]           3,840\n",
      "      BatchNorm2d-47             [-1, 40, 2, 2]              80\n",
      " InvertedResidual-48             [-1, 40, 2, 2]               0\n",
      "           Conv2d-49            [-1, 240, 2, 2]           9,600\n",
      "      BatchNorm2d-50            [-1, 240, 2, 2]             480\n",
      "        Hardswish-51            [-1, 240, 2, 2]               0\n",
      "           Conv2d-52            [-1, 240, 2, 2]           6,000\n",
      "      BatchNorm2d-53            [-1, 240, 2, 2]             480\n",
      "        Hardswish-54            [-1, 240, 2, 2]               0\n",
      "AdaptiveAvgPool2d-55            [-1, 240, 1, 1]               0\n",
      "           Conv2d-56             [-1, 64, 1, 1]          15,424\n",
      "             ReLU-57             [-1, 64, 1, 1]               0\n",
      "           Conv2d-58            [-1, 240, 1, 1]          15,600\n",
      "      Hardsigmoid-59            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-60            [-1, 240, 2, 2]               0\n",
      "           Conv2d-61             [-1, 40, 2, 2]           9,600\n",
      "      BatchNorm2d-62             [-1, 40, 2, 2]              80\n",
      " InvertedResidual-63             [-1, 40, 2, 2]               0\n",
      "           Conv2d-64            [-1, 240, 2, 2]           9,600\n",
      "      BatchNorm2d-65            [-1, 240, 2, 2]             480\n",
      "        Hardswish-66            [-1, 240, 2, 2]               0\n",
      "           Conv2d-67            [-1, 240, 2, 2]           6,000\n",
      "      BatchNorm2d-68            [-1, 240, 2, 2]             480\n",
      "        Hardswish-69            [-1, 240, 2, 2]               0\n",
      "AdaptiveAvgPool2d-70            [-1, 240, 1, 1]               0\n",
      "           Conv2d-71             [-1, 64, 1, 1]          15,424\n",
      "             ReLU-72             [-1, 64, 1, 1]               0\n",
      "           Conv2d-73            [-1, 240, 1, 1]          15,600\n",
      "      Hardsigmoid-74            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-75            [-1, 240, 2, 2]               0\n",
      "           Conv2d-76             [-1, 40, 2, 2]           9,600\n",
      "      BatchNorm2d-77             [-1, 40, 2, 2]              80\n",
      " InvertedResidual-78             [-1, 40, 2, 2]               0\n",
      "           Conv2d-79            [-1, 120, 2, 2]           4,800\n",
      "      BatchNorm2d-80            [-1, 120, 2, 2]             240\n",
      "        Hardswish-81            [-1, 120, 2, 2]               0\n",
      "           Conv2d-82            [-1, 120, 2, 2]           3,000\n",
      "      BatchNorm2d-83            [-1, 120, 2, 2]             240\n",
      "        Hardswish-84            [-1, 120, 2, 2]               0\n",
      "AdaptiveAvgPool2d-85            [-1, 120, 1, 1]               0\n",
      "           Conv2d-86             [-1, 32, 1, 1]           3,872\n",
      "             ReLU-87             [-1, 32, 1, 1]               0\n",
      "           Conv2d-88            [-1, 120, 1, 1]           3,960\n",
      "      Hardsigmoid-89            [-1, 120, 1, 1]               0\n",
      "SqueezeExcitation-90            [-1, 120, 2, 2]               0\n",
      "           Conv2d-91             [-1, 48, 2, 2]           5,760\n",
      "      BatchNorm2d-92             [-1, 48, 2, 2]              96\n",
      " InvertedResidual-93             [-1, 48, 2, 2]               0\n",
      "           Conv2d-94            [-1, 144, 2, 2]           6,912\n",
      "      BatchNorm2d-95            [-1, 144, 2, 2]             288\n",
      "        Hardswish-96            [-1, 144, 2, 2]               0\n",
      "           Conv2d-97            [-1, 144, 2, 2]           3,600\n",
      "      BatchNorm2d-98            [-1, 144, 2, 2]             288\n",
      "        Hardswish-99            [-1, 144, 2, 2]               0\n",
      "AdaptiveAvgPool2d-100            [-1, 144, 1, 1]               0\n",
      "          Conv2d-101             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-102             [-1, 40, 1, 1]               0\n",
      "          Conv2d-103            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-104            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-105            [-1, 144, 2, 2]               0\n",
      "          Conv2d-106             [-1, 48, 2, 2]           6,912\n",
      "     BatchNorm2d-107             [-1, 48, 2, 2]              96\n",
      "InvertedResidual-108             [-1, 48, 2, 2]               0\n",
      "          Conv2d-109            [-1, 288, 2, 2]          13,824\n",
      "     BatchNorm2d-110            [-1, 288, 2, 2]             576\n",
      "       Hardswish-111            [-1, 288, 2, 2]               0\n",
      "          Conv2d-112            [-1, 288, 1, 1]           7,200\n",
      "     BatchNorm2d-113            [-1, 288, 1, 1]             576\n",
      "       Hardswish-114            [-1, 288, 1, 1]               0\n",
      "AdaptiveAvgPool2d-115            [-1, 288, 1, 1]               0\n",
      "          Conv2d-116             [-1, 72, 1, 1]          20,808\n",
      "            ReLU-117             [-1, 72, 1, 1]               0\n",
      "          Conv2d-118            [-1, 288, 1, 1]          21,024\n",
      "     Hardsigmoid-119            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-120            [-1, 288, 1, 1]               0\n",
      "          Conv2d-121             [-1, 96, 1, 1]          27,648\n",
      "     BatchNorm2d-122             [-1, 96, 1, 1]             192\n",
      "InvertedResidual-123             [-1, 96, 1, 1]               0\n",
      "          Conv2d-124            [-1, 576, 1, 1]          55,296\n",
      "     BatchNorm2d-125            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-126            [-1, 576, 1, 1]               0\n",
      "          Conv2d-127            [-1, 576, 1, 1]          14,400\n",
      "     BatchNorm2d-128            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-129            [-1, 576, 1, 1]               0\n",
      "AdaptiveAvgPool2d-130            [-1, 576, 1, 1]               0\n",
      "          Conv2d-131            [-1, 144, 1, 1]          83,088\n",
      "            ReLU-132            [-1, 144, 1, 1]               0\n",
      "          Conv2d-133            [-1, 576, 1, 1]          83,520\n",
      "     Hardsigmoid-134            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-135            [-1, 576, 1, 1]               0\n",
      "          Conv2d-136             [-1, 96, 1, 1]          55,296\n",
      "     BatchNorm2d-137             [-1, 96, 1, 1]             192\n",
      "InvertedResidual-138             [-1, 96, 1, 1]               0\n",
      "          Conv2d-139            [-1, 576, 1, 1]          55,296\n",
      "     BatchNorm2d-140            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-141            [-1, 576, 1, 1]               0\n",
      "          Conv2d-142            [-1, 576, 1, 1]          14,400\n",
      "     BatchNorm2d-143            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-144            [-1, 576, 1, 1]               0\n",
      "AdaptiveAvgPool2d-145            [-1, 576, 1, 1]               0\n",
      "          Conv2d-146            [-1, 144, 1, 1]          83,088\n",
      "            ReLU-147            [-1, 144, 1, 1]               0\n",
      "          Conv2d-148            [-1, 576, 1, 1]          83,520\n",
      "     Hardsigmoid-149            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-150            [-1, 576, 1, 1]               0\n",
      "          Conv2d-151             [-1, 96, 1, 1]          55,296\n",
      "     BatchNorm2d-152             [-1, 96, 1, 1]             192\n",
      "InvertedResidual-153             [-1, 96, 1, 1]               0\n",
      "          Conv2d-154            [-1, 576, 1, 1]          55,296\n",
      "     BatchNorm2d-155            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-156            [-1, 576, 1, 1]               0\n",
      "AdaptiveAvgPool2d-157            [-1, 576, 1, 1]               0\n",
      "          Linear-158                 [-1, 1024]         590,848\n",
      "       Hardswish-159                 [-1, 1024]               0\n",
      "         Dropout-160                 [-1, 1024]               0\n",
      "          Linear-161                 [-1, 1000]       1,025,000\n",
      "================================================================\n",
      "Total params: 2,542,856\n",
      "Trainable params: 2,542,856\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.80\n",
      "Params size (MB): 9.70\n",
      "Estimated Total Size (MB): 10.51\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(),'orig.pth')\n",
    "torch.save(mode,'orig.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c1dd649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 16, 16]             432\n",
      "       BatchNorm2d-2           [-1, 16, 16, 16]              32\n",
      "         Hardswish-3           [-1, 16, 16, 16]               0\n",
      "            Conv2d-4             [-1, 16, 8, 8]             144\n",
      "       BatchNorm2d-5             [-1, 16, 8, 8]              32\n",
      "              ReLU-6             [-1, 16, 8, 8]               0\n",
      " AdaptiveAvgPool2d-7             [-1, 16, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]             136\n",
      "              ReLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10             [-1, 16, 1, 1]             144\n",
      "      Hardsigmoid-11             [-1, 16, 1, 1]               0\n",
      "SqueezeExcitation-12             [-1, 16, 8, 8]               0\n",
      "           Conv2d-13             [-1, 16, 8, 8]             256\n",
      "      BatchNorm2d-14             [-1, 16, 8, 8]              32\n",
      " InvertedResidual-15             [-1, 16, 8, 8]               0\n",
      "           Conv2d-16             [-1, 72, 8, 8]           1,152\n",
      "      BatchNorm2d-17             [-1, 72, 8, 8]             144\n",
      "             ReLU-18             [-1, 72, 8, 8]               0\n",
      "           Conv2d-19             [-1, 72, 4, 4]             648\n",
      "      BatchNorm2d-20             [-1, 72, 4, 4]             144\n",
      "             ReLU-21             [-1, 72, 4, 4]               0\n",
      "           Conv2d-22             [-1, 24, 4, 4]           1,728\n",
      "      BatchNorm2d-23             [-1, 24, 4, 4]              48\n",
      " InvertedResidual-24             [-1, 24, 4, 4]               0\n",
      "           Conv2d-25             [-1, 88, 4, 4]           2,112\n",
      "      BatchNorm2d-26             [-1, 88, 4, 4]             176\n",
      "             ReLU-27             [-1, 88, 4, 4]               0\n",
      "           Conv2d-28             [-1, 88, 4, 4]             792\n",
      "      BatchNorm2d-29             [-1, 88, 4, 4]             176\n",
      "             ReLU-30             [-1, 88, 4, 4]               0\n",
      "           Conv2d-31             [-1, 24, 4, 4]           2,112\n",
      "      BatchNorm2d-32             [-1, 24, 4, 4]              48\n",
      " InvertedResidual-33             [-1, 24, 4, 4]               0\n",
      "           Conv2d-34             [-1, 96, 4, 4]           2,304\n",
      "      BatchNorm2d-35             [-1, 96, 4, 4]             192\n",
      "        Hardswish-36             [-1, 96, 4, 4]               0\n",
      "           Conv2d-37             [-1, 96, 2, 2]           2,400\n",
      "      BatchNorm2d-38             [-1, 96, 2, 2]             192\n",
      "        Hardswish-39             [-1, 96, 2, 2]               0\n",
      "AdaptiveAvgPool2d-40             [-1, 96, 1, 1]               0\n",
      "           Conv2d-41             [-1, 24, 1, 1]           2,328\n",
      "             ReLU-42             [-1, 24, 1, 1]               0\n",
      "           Conv2d-43             [-1, 96, 1, 1]           2,400\n",
      "      Hardsigmoid-44             [-1, 96, 1, 1]               0\n",
      "SqueezeExcitation-45             [-1, 96, 2, 2]               0\n",
      "           Conv2d-46             [-1, 40, 2, 2]           3,840\n",
      "      BatchNorm2d-47             [-1, 40, 2, 2]              80\n",
      " InvertedResidual-48             [-1, 40, 2, 2]               0\n",
      "           Conv2d-49            [-1, 240, 2, 2]           9,600\n",
      "      BatchNorm2d-50            [-1, 240, 2, 2]             480\n",
      "        Hardswish-51            [-1, 240, 2, 2]               0\n",
      "           Conv2d-52            [-1, 240, 2, 2]           6,000\n",
      "      BatchNorm2d-53            [-1, 240, 2, 2]             480\n",
      "        Hardswish-54            [-1, 240, 2, 2]               0\n",
      "AdaptiveAvgPool2d-55            [-1, 240, 1, 1]               0\n",
      "           Conv2d-56             [-1, 64, 1, 1]          15,424\n",
      "             ReLU-57             [-1, 64, 1, 1]               0\n",
      "           Conv2d-58            [-1, 240, 1, 1]          15,600\n",
      "      Hardsigmoid-59            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-60            [-1, 240, 2, 2]               0\n",
      "           Conv2d-61             [-1, 40, 2, 2]           9,600\n",
      "      BatchNorm2d-62             [-1, 40, 2, 2]              80\n",
      " InvertedResidual-63             [-1, 40, 2, 2]               0\n",
      "           Conv2d-64            [-1, 240, 2, 2]           9,600\n",
      "      BatchNorm2d-65            [-1, 240, 2, 2]             480\n",
      "        Hardswish-66            [-1, 240, 2, 2]               0\n",
      "           Conv2d-67            [-1, 240, 2, 2]           6,000\n",
      "      BatchNorm2d-68            [-1, 240, 2, 2]             480\n",
      "        Hardswish-69            [-1, 240, 2, 2]               0\n",
      "AdaptiveAvgPool2d-70            [-1, 240, 1, 1]               0\n",
      "           Conv2d-71             [-1, 64, 1, 1]          15,424\n",
      "             ReLU-72             [-1, 64, 1, 1]               0\n",
      "           Conv2d-73            [-1, 240, 1, 1]          15,600\n",
      "      Hardsigmoid-74            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-75            [-1, 240, 2, 2]               0\n",
      "           Conv2d-76             [-1, 40, 2, 2]           9,600\n",
      "      BatchNorm2d-77             [-1, 40, 2, 2]              80\n",
      " InvertedResidual-78             [-1, 40, 2, 2]               0\n",
      "           Conv2d-79            [-1, 120, 2, 2]           4,800\n",
      "      BatchNorm2d-80            [-1, 120, 2, 2]             240\n",
      "        Hardswish-81            [-1, 120, 2, 2]               0\n",
      "           Conv2d-82            [-1, 120, 2, 2]           3,000\n",
      "      BatchNorm2d-83            [-1, 120, 2, 2]             240\n",
      "        Hardswish-84            [-1, 120, 2, 2]               0\n",
      "AdaptiveAvgPool2d-85            [-1, 120, 1, 1]               0\n",
      "           Conv2d-86             [-1, 32, 1, 1]           3,872\n",
      "             ReLU-87             [-1, 32, 1, 1]               0\n",
      "           Conv2d-88            [-1, 120, 1, 1]           3,960\n",
      "      Hardsigmoid-89            [-1, 120, 1, 1]               0\n",
      "SqueezeExcitation-90            [-1, 120, 2, 2]               0\n",
      "           Conv2d-91             [-1, 48, 2, 2]           5,760\n",
      "      BatchNorm2d-92             [-1, 48, 2, 2]              96\n",
      " InvertedResidual-93             [-1, 48, 2, 2]               0\n",
      "           Conv2d-94            [-1, 144, 2, 2]           6,912\n",
      "      BatchNorm2d-95            [-1, 144, 2, 2]             288\n",
      "        Hardswish-96            [-1, 144, 2, 2]               0\n",
      "           Conv2d-97            [-1, 144, 2, 2]           3,600\n",
      "      BatchNorm2d-98            [-1, 144, 2, 2]             288\n",
      "        Hardswish-99            [-1, 144, 2, 2]               0\n",
      "AdaptiveAvgPool2d-100            [-1, 144, 1, 1]               0\n",
      "          Conv2d-101             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-102             [-1, 40, 1, 1]               0\n",
      "          Conv2d-103            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-104            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-105            [-1, 144, 2, 2]               0\n",
      "          Conv2d-106             [-1, 48, 2, 2]           6,912\n",
      "     BatchNorm2d-107             [-1, 48, 2, 2]              96\n",
      "InvertedResidual-108             [-1, 48, 2, 2]               0\n",
      "          Conv2d-109            [-1, 288, 2, 2]          13,824\n",
      "     BatchNorm2d-110            [-1, 288, 2, 2]             576\n",
      "       Hardswish-111            [-1, 288, 2, 2]               0\n",
      "          Conv2d-112            [-1, 288, 1, 1]           7,200\n",
      "     BatchNorm2d-113            [-1, 288, 1, 1]             576\n",
      "       Hardswish-114            [-1, 288, 1, 1]               0\n",
      "AdaptiveAvgPool2d-115            [-1, 288, 1, 1]               0\n",
      "          Conv2d-116             [-1, 72, 1, 1]          20,808\n",
      "            ReLU-117             [-1, 72, 1, 1]               0\n",
      "          Conv2d-118            [-1, 288, 1, 1]          21,024\n",
      "     Hardsigmoid-119            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-120            [-1, 288, 1, 1]               0\n",
      "          Conv2d-121             [-1, 96, 1, 1]          27,648\n",
      "     BatchNorm2d-122             [-1, 96, 1, 1]             192\n",
      "InvertedResidual-123             [-1, 96, 1, 1]               0\n",
      "          Conv2d-124            [-1, 576, 1, 1]          55,296\n",
      "     BatchNorm2d-125            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-126            [-1, 576, 1, 1]               0\n",
      "          Conv2d-127            [-1, 576, 1, 1]          14,400\n",
      "     BatchNorm2d-128            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-129            [-1, 576, 1, 1]               0\n",
      "AdaptiveAvgPool2d-130            [-1, 576, 1, 1]               0\n",
      "          Conv2d-131            [-1, 144, 1, 1]          83,088\n",
      "            ReLU-132            [-1, 144, 1, 1]               0\n",
      "          Conv2d-133            [-1, 576, 1, 1]          83,520\n",
      "     Hardsigmoid-134            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-135            [-1, 576, 1, 1]               0\n",
      "          Conv2d-136             [-1, 96, 1, 1]          55,296\n",
      "     BatchNorm2d-137             [-1, 96, 1, 1]             192\n",
      "InvertedResidual-138             [-1, 96, 1, 1]               0\n",
      "          Conv2d-139            [-1, 576, 1, 1]          55,296\n",
      "     BatchNorm2d-140            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-141            [-1, 576, 1, 1]               0\n",
      "          Conv2d-142            [-1, 576, 1, 1]          14,400\n",
      "     BatchNorm2d-143            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-144            [-1, 576, 1, 1]               0\n",
      "AdaptiveAvgPool2d-145            [-1, 576, 1, 1]               0\n",
      "          Conv2d-146            [-1, 144, 1, 1]          83,088\n",
      "            ReLU-147            [-1, 144, 1, 1]               0\n",
      "          Conv2d-148            [-1, 576, 1, 1]          83,520\n",
      "     Hardsigmoid-149            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-150            [-1, 576, 1, 1]               0\n",
      "          Conv2d-151             [-1, 96, 1, 1]          55,296\n",
      "     BatchNorm2d-152             [-1, 96, 1, 1]             192\n",
      "InvertedResidual-153             [-1, 96, 1, 1]               0\n",
      "          Conv2d-154            [-1, 576, 1, 1]          55,296\n",
      "     BatchNorm2d-155            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-156            [-1, 576, 1, 1]               0\n",
      "AdaptiveAvgPool2d-157            [-1, 576, 1, 1]               0\n",
      "          Linear-158                 [-1, 1024]         590,848\n",
      "       Hardswish-159                 [-1, 1024]               0\n",
      "         Dropout-160                 [-1, 1024]               0\n",
      "          Linear-161                 [-1, 1000]       1,025,000\n",
      "================================================================\n",
      "Total params: 2,542,856\n",
      "Trainable params: 2,542,856\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.80\n",
      "Params size (MB): 9.70\n",
      "Estimated Total Size (MB): 10.51\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(pruned_model,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e31cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
